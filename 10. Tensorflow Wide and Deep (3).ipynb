{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UCI](http://mlr.cs.umass.edu/ml/assets/logo.gif)\n",
    "\n",
    "# Loading in the Data\n",
    "In this example, we are going to use crossed columns and embedding columns inside of a tensorflow object created with the contrib \"learn\" library.\n",
    "\n",
    "However, we will start the process by loading up a dataset with a mix of categorical data and numeric data. This dataset is quite old and has been used many times in machine learning examples: the census data from 1990's. We will use it to predict if a person will earn over or under 50k per year.\n",
    "\n",
    "- https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Music</th>\n",
       "      <th>Dance</th>\n",
       "      <th>Folk</th>\n",
       "      <th>Country</th>\n",
       "      <th>Classical music</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Pop</th>\n",
       "      <th>Rock</th>\n",
       "      <th>Metal or Hardrock</th>\n",
       "      <th>Punk</th>\n",
       "      <th>...</th>\n",
       "      <th>Spending on gadgets</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Number of siblings</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Left - right handed</th>\n",
       "      <th>Education</th>\n",
       "      <th>Village - town</th>\n",
       "      <th>House - block of flats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>village</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>left handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>primary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>left handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>left handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>left handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>male</td>\n",
       "      <td>left handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>left handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>masters degree</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>masters degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>NaN</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>NaN</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>masters degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>16.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>primary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>primary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>male</td>\n",
       "      <td>left handed</td>\n",
       "      <td>masters degree</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Music  Dance  Folk  Country  Classical music  Musical  Pop  Rock  \\\n",
       "0       5.0    2.0   1.0      2.0              2.0      1.0  5.0   5.0   \n",
       "1       4.0    2.0   1.0      1.0              1.0      2.0  3.0   5.0   \n",
       "2       5.0    2.0   2.0      3.0              4.0      5.0  3.0   5.0   \n",
       "3       5.0    2.0   1.0      1.0              1.0      1.0  2.0   2.0   \n",
       "4       5.0    4.0   3.0      2.0              4.0      3.0  5.0   3.0   \n",
       "5       5.0    2.0   3.0      2.0              3.0      3.0  2.0   5.0   \n",
       "6       5.0    5.0   3.0      1.0              2.0      2.0  5.0   3.0   \n",
       "7       5.0    3.0   2.0      1.0              2.0      2.0  4.0   5.0   \n",
       "8       5.0    3.0   1.0      1.0              2.0      4.0  3.0   5.0   \n",
       "9       5.0    2.0   5.0      2.0              2.0      5.0  3.0   5.0   \n",
       "10      5.0    3.0   2.0      1.0              2.0      3.0  4.0   3.0   \n",
       "11      5.0    1.0   1.0      1.0              4.0      1.0  2.0   5.0   \n",
       "12      5.0    1.0   2.0      1.0              4.0      3.0  3.0   5.0   \n",
       "13      5.0    5.0   3.0      2.0              1.0      5.0  5.0   2.0   \n",
       "14      5.0    2.0   1.0      1.0              2.0      3.0  4.0   5.0   \n",
       "15      1.0    2.0   2.0      3.0              4.0      3.0  3.0   5.0   \n",
       "16      5.0    3.0   1.0      1.0              1.0      2.0  4.0   4.0   \n",
       "17      5.0    3.0   3.0      3.0              2.0      2.0  4.0   4.0   \n",
       "18      5.0    5.0   4.0      3.0              4.0      5.0  5.0   4.0   \n",
       "19      5.0    3.0   3.0      2.0              4.0      2.0  2.0   4.0   \n",
       "20      5.0    3.0   2.0      3.0              4.0      3.0  2.0   5.0   \n",
       "21      5.0    1.0   1.0      3.0              2.0      2.0  2.0   5.0   \n",
       "22      5.0    3.0   2.0      3.0              3.0      3.0  4.0   NaN   \n",
       "23      5.0    4.0   2.0      2.0              2.0      4.0  4.0   5.0   \n",
       "24      5.0    3.0   1.0      1.0              4.0      3.0  3.0   5.0   \n",
       "25      5.0    4.0   2.0      1.0              2.0      3.0  5.0   1.0   \n",
       "26      5.0    5.0   5.0      4.0              5.0      3.0  4.0   4.0   \n",
       "27      4.0    3.0   4.0      1.0              3.0      2.0  2.0   4.0   \n",
       "28      5.0    5.0   1.0      1.0              1.0      1.0  3.0   4.0   \n",
       "29      5.0    3.0   4.0      2.0              3.0      3.0  3.0   4.0   \n",
       "...     ...    ...   ...      ...              ...      ...  ...   ...   \n",
       "980     5.0    2.0   2.0      2.0              5.0      1.0  4.0   4.0   \n",
       "981     5.0    1.0   2.0      4.0              5.0      5.0  3.0   4.0   \n",
       "982     5.0    2.0   1.0      1.0              3.0      2.0  2.0   4.0   \n",
       "983     5.0    3.0   1.0      1.0              3.0      3.0  5.0   3.0   \n",
       "984     5.0    1.0   3.0      2.0              3.0      3.0  3.0   4.0   \n",
       "985     5.0    1.0   3.0      2.0              4.0      2.0  4.0   5.0   \n",
       "986     4.0    2.0   5.0      3.0              5.0      2.0  2.0   4.0   \n",
       "987     4.0    2.0   2.0      3.0              2.0      1.0  1.0   5.0   \n",
       "988     5.0    3.0   2.0      2.0              5.0      2.0  2.0   5.0   \n",
       "989     5.0    3.0   2.0      2.0              4.0      5.0  5.0   5.0   \n",
       "990     5.0    2.0   1.0      1.0              2.0      2.0  3.0   4.0   \n",
       "991     5.0    2.0   1.0      1.0              1.0      1.0  1.0   5.0   \n",
       "992     4.0    4.0   1.0      4.0              4.0      1.0  3.0   4.0   \n",
       "993     5.0    3.0   1.0      2.0              3.0      1.0  4.0   4.0   \n",
       "994     5.0    3.0   2.0      2.0              3.0      4.0  3.0   5.0   \n",
       "995     5.0    2.0   1.0      1.0              1.0      2.0  1.0   4.0   \n",
       "996     5.0    1.0   1.0      1.0              3.0      2.0  1.0   4.0   \n",
       "997     5.0    3.0   3.0      3.0              1.0      3.0  3.0   4.0   \n",
       "998     5.0    4.0   3.0      1.0              3.0      2.0  3.0   5.0   \n",
       "999     5.0    5.0   3.0      4.0              5.0      4.0  4.0   4.0   \n",
       "1000    5.0    3.0   3.0      2.0              3.0      3.0  4.0   3.0   \n",
       "1001    5.0    5.0   1.0      2.0              1.0      4.0  4.0   3.0   \n",
       "1002    5.0    3.0   1.0      3.0              1.0      1.0  2.0   5.0   \n",
       "1003    4.0    4.0   3.0      2.0              2.0      3.0  4.0   5.0   \n",
       "1004    5.0    4.0   1.0      2.0              3.0      2.0  3.0   3.0   \n",
       "1005    5.0    5.0   2.0      2.0              5.0      4.0  4.0   4.0   \n",
       "1006    4.0    5.0   1.0      3.0              4.0      1.0  4.0   1.0   \n",
       "1007    4.0    1.0   1.0      2.0              2.0      2.0  3.0   4.0   \n",
       "1008    5.0    3.0   3.0      1.0              3.0      1.0  3.0   4.0   \n",
       "1009    5.0    4.0   3.0      2.0              3.0      3.0  4.0   1.0   \n",
       "\n",
       "      Metal or Hardrock  Punk           ...            Spending on gadgets  \\\n",
       "0                   1.0   1.0           ...                              1   \n",
       "1                   4.0   4.0           ...                              5   \n",
       "2                   3.0   4.0           ...                              4   \n",
       "3                   1.0   4.0           ...                              4   \n",
       "4                   1.0   2.0           ...                              2   \n",
       "5                   5.0   3.0           ...                              4   \n",
       "6                   1.0   1.0           ...                              1   \n",
       "7                   1.0   2.0           ...                              3   \n",
       "8                   5.0   1.0           ...                              3   \n",
       "9                   2.0   3.0           ...                              2   \n",
       "10                  2.0   1.0           ...                              2   \n",
       "11                  1.0   1.0           ...                              1   \n",
       "12                  4.0   2.0           ...                              2   \n",
       "13                  1.0   1.0           ...                              3   \n",
       "14                  2.0   5.0           ...                              1   \n",
       "15                  5.0   5.0           ...                              5   \n",
       "16                  1.0   3.0           ...                              3   \n",
       "17                  2.0   3.0           ...                              2   \n",
       "18                  4.0   3.0           ...                              5   \n",
       "19                  5.0   2.0           ...                              3   \n",
       "20                  5.0   4.0           ...                              4   \n",
       "21                  5.0   4.0           ...                              2   \n",
       "22                  1.0   2.0           ...                              4   \n",
       "23                  2.0   3.0           ...                              2   \n",
       "24                  5.0   5.0           ...                              5   \n",
       "25                  1.0   1.0           ...                              1   \n",
       "26                  3.0   2.0           ...                              4   \n",
       "27                  2.0   4.0           ...                              2   \n",
       "28                  1.0   3.0           ...                              1   \n",
       "29                  1.0   3.0           ...                              2   \n",
       "...                 ...   ...           ...                            ...   \n",
       "980                 4.0   2.0           ...                              4   \n",
       "981                 1.0   1.0           ...                              2   \n",
       "982                 3.0   2.0           ...                              3   \n",
       "983                 1.0   1.0           ...                              3   \n",
       "984                 4.0   3.0           ...                              4   \n",
       "985                 3.0   3.0           ...                              3   \n",
       "986                 2.0   4.0           ...                              1   \n",
       "987                 3.0   2.0           ...                              4   \n",
       "988                 2.0   4.0           ...                              5   \n",
       "989                 5.0   1.0           ...                              1   \n",
       "990                 4.0   1.0           ...                              4   \n",
       "991                 5.0   4.0           ...                              3   \n",
       "992                 4.0   3.0           ...                              1   \n",
       "993                 1.0   1.0           ...                              4   \n",
       "994                 3.0   4.0           ...                              3   \n",
       "995                 4.0   3.0           ...                              2   \n",
       "996                 3.0   1.0           ...                              3   \n",
       "997                 2.0   4.0           ...                              2   \n",
       "998                 5.0   4.0           ...                              2   \n",
       "999                 3.0   NaN           ...                              3   \n",
       "1000                1.0   1.0           ...                              4   \n",
       "1001                2.0   4.0           ...                              2   \n",
       "1002                5.0   3.0           ...                              3   \n",
       "1003                2.0   3.0           ...                              4   \n",
       "1004                4.0   3.0           ...                              4   \n",
       "1005                3.0   2.0           ...                              3   \n",
       "1006                1.0   4.0           ...                              5   \n",
       "1007                1.0   2.0           ...                              2   \n",
       "1008                1.0   1.0           ...                              3   \n",
       "1009                1.0   2.0           ...                              1   \n",
       "\n",
       "       Age  Height  Weight  Number of siblings  Gender  Left - right handed  \\\n",
       "0     20.0   163.0    48.0                 1.0  female         right handed   \n",
       "1     19.0   163.0    58.0                 2.0  female         right handed   \n",
       "2     20.0   176.0    67.0                 2.0  female         right handed   \n",
       "3     22.0   172.0    59.0                 1.0  female         right handed   \n",
       "4     20.0   170.0    59.0                 1.0  female         right handed   \n",
       "5     20.0   186.0    77.0                 1.0    male         right handed   \n",
       "6     20.0   177.0    50.0                 1.0  female         right handed   \n",
       "7     19.0   184.0    90.0                 1.0    male         right handed   \n",
       "8     18.0   166.0    55.0                 1.0  female         right handed   \n",
       "9     19.0   174.0    60.0                 3.0  female         right handed   \n",
       "10    19.0   175.0    60.0                 2.0  female          left handed   \n",
       "11    17.0   176.0    60.0                 1.0  female         right handed   \n",
       "12    24.0   168.0    55.0                10.0  female         right handed   \n",
       "13    19.0   165.0    55.0                 1.0  female         right handed   \n",
       "14    22.0   175.0    57.0                 1.0  female         right handed   \n",
       "15    18.0   177.0    77.0                 0.0    male         right handed   \n",
       "16    19.0   175.0    65.0                 2.0  female         right handed   \n",
       "17    20.0   168.0    65.0                 1.0  female         right handed   \n",
       "18    18.0   181.0    78.0                 2.0    male         right handed   \n",
       "19    18.0   188.0    90.0                 1.0    male          left handed   \n",
       "20    20.0   186.0    77.0                 1.0    male         right handed   \n",
       "21    24.0   186.0    85.0                 1.0    male          left handed   \n",
       "22    22.0   167.0    70.0                 1.0  female         right handed   \n",
       "23    20.0   168.0    54.0                 3.0  female         right handed   \n",
       "24    19.0   167.0    51.0                 1.0  female         right handed   \n",
       "25    20.0   170.0    56.0                 1.0  female         right handed   \n",
       "26    22.0   165.0    63.0                 1.0  female          left handed   \n",
       "27    19.0   163.0    50.0                 1.0  female         right handed   \n",
       "28    20.0   187.0    80.0                 2.0    male          left handed   \n",
       "29    19.0   168.0    60.0                 2.0  female         right handed   \n",
       "...    ...     ...     ...                 ...     ...                  ...   \n",
       "980   18.0   166.0    53.0                 2.0  female         right handed   \n",
       "981   19.0   168.0    57.0                 1.0  female          left handed   \n",
       "982   18.0   178.0    70.0                 2.0    male         right handed   \n",
       "983   22.0   172.0    60.0                 2.0  female         right handed   \n",
       "984   21.0   165.0    55.0                 1.0  female         right handed   \n",
       "985   20.0   167.0    53.0                 1.0  female         right handed   \n",
       "986   19.0   161.0    60.0                 3.0  female         right handed   \n",
       "987   20.0   184.0    80.0                 1.0    male         right handed   \n",
       "988   19.0   183.0    70.0                 0.0    male         right handed   \n",
       "989   30.0   168.0    54.0                 2.0  female         right handed   \n",
       "990   29.0   180.0    93.0                 1.0    male         right handed   \n",
       "991   21.0   172.0    80.0                 0.0    male         right handed   \n",
       "992   30.0   200.0   150.0                 1.0    male         right handed   \n",
       "993   21.0   173.0    55.0                 0.0  female         right handed   \n",
       "994   20.0   176.0    78.0                 0.0  female         right handed   \n",
       "995   18.0   163.0    53.0                 0.0  female         right handed   \n",
       "996   20.0   190.0    74.0                 0.0    male         right handed   \n",
       "997   19.0   167.0    53.0                 1.0  female         right handed   \n",
       "998   28.0   192.0   105.0                 1.0    male         right handed   \n",
       "999   19.0   181.0    70.0                 1.0    male         right handed   \n",
       "1000  16.0   153.0    62.0                 1.0  female         right handed   \n",
       "1001  18.0   160.0    63.0                 2.0  female         right handed   \n",
       "1002  22.0   181.0    72.0                 1.0    male         right handed   \n",
       "1003  20.0   172.0    63.0                 1.0  female         right handed   \n",
       "1004  22.0   175.0    60.0                 1.0    male         right handed   \n",
       "1005  20.0   164.0    57.0                 1.0  female         right handed   \n",
       "1006  27.0   183.0    80.0                 5.0    male          left handed   \n",
       "1007  18.0   173.0    75.0                 0.0  female         right handed   \n",
       "1008  25.0   173.0    58.0                 1.0  female         right handed   \n",
       "1009  21.0   185.0    72.0                 1.0    male         right handed   \n",
       "\n",
       "                    Education  Village - town  House - block of flats  \n",
       "0     college/bachelor degree         village          block of flats  \n",
       "1     college/bachelor degree            city          block of flats  \n",
       "2            secondary school            city          block of flats  \n",
       "3     college/bachelor degree            city          house/bungalow  \n",
       "4            secondary school         village          house/bungalow  \n",
       "5            secondary school            city          block of flats  \n",
       "6            secondary school         village          house/bungalow  \n",
       "7     college/bachelor degree            city          house/bungalow  \n",
       "8            secondary school            city          house/bungalow  \n",
       "9            secondary school            city          block of flats  \n",
       "10           secondary school            city          block of flats  \n",
       "11             primary school            city          block of flats  \n",
       "12    college/bachelor degree            city          block of flats  \n",
       "13           secondary school            city          block of flats  \n",
       "14    college/bachelor degree            city          block of flats  \n",
       "15           secondary school            city          block of flats  \n",
       "16    college/bachelor degree            city          block of flats  \n",
       "17           secondary school         village          house/bungalow  \n",
       "18           secondary school            city          house/bungalow  \n",
       "19           secondary school            city          house/bungalow  \n",
       "20           secondary school            city          block of flats  \n",
       "21           secondary school            city          block of flats  \n",
       "22    college/bachelor degree            city          house/bungalow  \n",
       "23           secondary school            city          block of flats  \n",
       "24           secondary school            city          block of flats  \n",
       "25           secondary school            city          block of flats  \n",
       "26    college/bachelor degree            city          house/bungalow  \n",
       "27           secondary school            city          house/bungalow  \n",
       "28           secondary school         village          house/bungalow  \n",
       "29           secondary school         village          block of flats  \n",
       "...                       ...             ...                     ...  \n",
       "980          secondary school            city          house/bungalow  \n",
       "981          secondary school         village          house/bungalow  \n",
       "982          secondary school            city          house/bungalow  \n",
       "983          secondary school         village          house/bungalow  \n",
       "984          secondary school            city          block of flats  \n",
       "985          secondary school            city          block of flats  \n",
       "986          secondary school            city          house/bungalow  \n",
       "987          secondary school            city          block of flats  \n",
       "988          secondary school            city          block of flats  \n",
       "989            masters degree         village          house/bungalow  \n",
       "990          secondary school            city          block of flats  \n",
       "991          secondary school            city          house/bungalow  \n",
       "992            masters degree            city          block of flats  \n",
       "993          secondary school            city          block of flats  \n",
       "994          secondary school            city          block of flats  \n",
       "995          secondary school            city          block of flats  \n",
       "996          secondary school             NaN          block of flats  \n",
       "997          secondary school             NaN          block of flats  \n",
       "998            masters degree            city          block of flats  \n",
       "999          secondary school            city          block of flats  \n",
       "1000           primary school            city          block of flats  \n",
       "1001           primary school            city          block of flats  \n",
       "1002         secondary school            city          block of flats  \n",
       "1003         secondary school            city          house/bungalow  \n",
       "1004         secondary school            city          block of flats  \n",
       "1005         secondary school            city          house/bungalow  \n",
       "1006           masters degree         village          house/bungalow  \n",
       "1007         secondary school            city          block of flats  \n",
       "1008  college/bachelor degree            city          block of flats  \n",
       "1009         secondary school         village          house/bungalow  \n",
       "\n",
       "[1010 rows x 75 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.special import expit\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from memory_profiler import memory_usage\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "target_classifier = 'Spending on gadgets'\n",
    "df = pd.read_csv('responses.csv', sep=\",\")\n",
    "desired_features = [\"Music\",\"Dance\",\"Folk\",\"Country\",\"Classical music\",\"Musical\",\n",
    "    \"Pop\",\"Rock\",\"Metal or Hardrock\",\"Punk\",\"Hiphop, Rap\",\"Reggae, Ska\",\n",
    "    \"Swing, Jazz\",\"Rock n roll\",\"Alternative\",\"Latino\",\"Techno, Trance\",\n",
    "    \"Opera\",\"Movies\",\"Horror\",\"Thriller\",\"Comedy\",\"Romantic\",\"Sci-fi\",\"War\",\n",
    "    \"Fantasy/Fairy tales\",\"Animated\",\"Documentary\",\"Western\",\"Action\",\"History\",\n",
    "    \"Psychology\",\"Politics\",\"Mathematics\",\"Physics\",\"Internet\",\"PC\",\"Economy Management\",\n",
    "    \"Biology\",\"Chemistry\",\"Reading\",\"Geography\",\"Foreign languages\",\"Medicine\",\"Law\",\n",
    "    \"Cars\",\"Art exhibitions\",\"Religion\",\"Countryside, outdoors\",\"Dancing\",\n",
    "    \"Musical instruments\",\"Writing\",\"Passive sport\",\"Active sport\",\"Gardening\",\"Celebrities\",\n",
    "    \"Shopping\",\"Science and technology\",\"Theatre\",\"Fun with friends\",\"Adrenaline sports\",\n",
    "    \"Pets\",\"Smoking\",\"Alcohol\",\"Healthy eating\",\"Spending on gadgets\",\"Age\",\"Height\",\"Weight\",\n",
    "    \"Number of siblings\",\"Gender\",\"Left - right handed\",\"Education\",\"Village - town\",\"House - block of flats\"\n",
    "]\n",
    "\n",
    "df = df[desired_features]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       5\n",
      "2       4\n",
      "3       4\n",
      "4       2\n",
      "5       4\n",
      "6       1\n",
      "7       3\n",
      "8       3\n",
      "9       2\n",
      "10      2\n",
      "11      1\n",
      "12      2\n",
      "13      3\n",
      "14      1\n",
      "15      5\n",
      "16      3\n",
      "17      2\n",
      "18      5\n",
      "19      3\n",
      "20      4\n",
      "21      2\n",
      "22      4\n",
      "23      2\n",
      "24      5\n",
      "25      1\n",
      "26      4\n",
      "27      2\n",
      "28      1\n",
      "29      2\n",
      "       ..\n",
      "980     4\n",
      "981     2\n",
      "982     3\n",
      "983     3\n",
      "984     4\n",
      "985     3\n",
      "986     1\n",
      "987     4\n",
      "988     5\n",
      "989     1\n",
      "990     4\n",
      "991     3\n",
      "992     1\n",
      "993     4\n",
      "994     3\n",
      "995     2\n",
      "996     3\n",
      "997     2\n",
      "998     2\n",
      "999     3\n",
      "1000    4\n",
      "1001    2\n",
      "1002    3\n",
      "1003    4\n",
      "1004    4\n",
      "1005    3\n",
      "1006    5\n",
      "1007    2\n",
      "1008    3\n",
      "1009    1\n",
      "Name: Spending on gadgets, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Spending on gadgets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'Reggae, Ska': 'ReggaeSka', 'Swing, Jazz': 'SwingJazz', 'Classical music': 'Classicalmusic', 'Metal or Hardrock': 'MetalorHardrock', 'Hiphop, Rap': 'HiphopRap', 'Rock n roll': 'Rocknroll', 'Techno, Trance': 'TechnoTrance', 'Sci-fi': 'Scifi', 'Fantasy/Fairy tales': 'Fantasy', 'Economy Management': 'EconomyManagement', 'Foreign languages': 'Foreignlanguages', 'Art exhibitions': 'Artexhibitions', 'Countryside, outdoors': 'outdoors', 'Musical instruments': 'Musicalinstruments', 'Passive sport': 'Passivesport', 'Active sport': 'Activesport', 'Science and technology': 'Scienceandtechnology', 'Fun with friends': 'Funwithfriends', 'Adrenaline sports': 'Adrenalinesports', 'Healthy eating': 'Healthyeating', 'Spending on gadgets': 'Spendingongadgets', 'Number of siblings': 'Numberofsiblings', 'Left - right handed': 'Leftrighthanded', 'Village - town': 'Villagetown', 'House - block of flats': 'Houseblockofflats'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "df_train = deepcopy(df)\n",
    "df_test = deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Music</th>\n",
       "      <th>Dance</th>\n",
       "      <th>Folk</th>\n",
       "      <th>Country</th>\n",
       "      <th>Classicalmusic</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Pop</th>\n",
       "      <th>Rock</th>\n",
       "      <th>MetalorHardrock</th>\n",
       "      <th>Punk</th>\n",
       "      <th>...</th>\n",
       "      <th>Spendingongadgets</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Numberofsiblings</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Leftrighthanded</th>\n",
       "      <th>Education</th>\n",
       "      <th>Villagetown</th>\n",
       "      <th>Houseblockofflats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>village</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Music  Dance  Folk  Country  Classicalmusic  Musical  Pop  Rock  \\\n",
       "0    5.0    2.0   1.0      2.0             2.0      1.0  5.0   5.0   \n",
       "1    4.0    2.0   1.0      1.0             1.0      2.0  3.0   5.0   \n",
       "2    5.0    2.0   2.0      3.0             4.0      5.0  3.0   5.0   \n",
       "3    5.0    2.0   1.0      1.0             1.0      1.0  2.0   2.0   \n",
       "4    5.0    4.0   3.0      2.0             4.0      3.0  5.0   3.0   \n",
       "\n",
       "   MetalorHardrock  Punk        ...          Spendingongadgets   Age  Height  \\\n",
       "0              1.0   1.0        ...                          1  20.0   163.0   \n",
       "1              4.0   4.0        ...                          5  19.0   163.0   \n",
       "2              3.0   4.0        ...                          4  20.0   176.0   \n",
       "3              1.0   4.0        ...                          4  22.0   172.0   \n",
       "4              1.0   2.0        ...                          2  20.0   170.0   \n",
       "\n",
       "   Weight  Numberofsiblings  Gender  Leftrighthanded                Education  \\\n",
       "0    48.0               1.0  female     right handed  college/bachelor degree   \n",
       "1    58.0               2.0  female     right handed  college/bachelor degree   \n",
       "2    67.0               2.0  female     right handed         secondary school   \n",
       "3    59.0               1.0  female     right handed  college/bachelor degree   \n",
       "4    59.0               1.0  female     right handed         secondary school   \n",
       "\n",
       "   Villagetown  Houseblockofflats  \n",
       "0      village     block of flats  \n",
       "1         city     block of flats  \n",
       "2         city     block of flats  \n",
       "3         city     house/bungalow  \n",
       "4      village     house/bungalow  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# let's just get rid of rows with any missing data\n",
    "# and then reset the indices of the dataframe so it corresponds to row number\n",
    "df_train.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.reset_index()\n",
    "\n",
    "df_test.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index()\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "For preprocessing, we are going to fix a few issues in the dataset. \n",
    "\n",
    "- This first includes the use of \"50K.\" instead of \"50K\" in the test set. \n",
    "- Next, we will encode the categorical features as integers (later on we will encode one hot)\n",
    "- Finally, we will make certain all the continuous data is scaled properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_headers = list(df.select_dtypes(include=['object']).columns)\n",
    "numeric_headers = list(df.select_dtypes(include=['float']).columns) + list(df.select_dtypes(include=['int']).columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Music', 'Dance', 'Folk', 'Country', 'Classicalmusic', 'Musical', 'Pop', 'Rock', 'MetalorHardrock', 'Punk', 'HiphopRap', 'ReggaeSka', 'SwingJazz', 'Rocknroll', 'Alternative', 'Latino', 'TechnoTrance', 'Opera', 'Movies', 'Horror', 'Thriller', 'Comedy', 'Romantic', 'Scifi', 'War', 'Fantasy', 'Animated', 'Documentary', 'Western', 'Action', 'History', 'Psychology', 'Politics', 'Mathematics', 'Physics', 'Internet', 'PC', 'EconomyManagement', 'Biology', 'Chemistry', 'Reading', 'Geography', 'Foreignlanguages', 'Medicine', 'Law', 'Cars', 'Artexhibitions', 'Religion', 'outdoors', 'Dancing', 'Musicalinstruments', 'Writing', 'Passivesport', 'Activesport', 'Gardening', 'Celebrities', 'Shopping', 'Scienceandtechnology', 'Theatre', 'Funwithfriends', 'Adrenalinesports', 'Pets', 'Healthyeating', 'Age', 'Height', 'Weight', 'Numberofsiblings', 'Spendingongadgets']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "encoders = dict() \n",
    "\n",
    "for col in categorical_headers:\n",
    "    df_train[col] = df_train[col].str.strip()\n",
    "    df_test[col] = df_test[col].str.strip()\n",
    "    \n",
    "    if col=='Spendingongadgets':\n",
    "        tmp = LabelEncoder()\n",
    "        df_train[col] = tmp.fit_transform(df_train[col])\n",
    "        df_test[col] = tmp.transform(df_test[col])\n",
    "    else:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df_train[col+'_int'] = encoders[col].fit_transform(df_train[col])\n",
    "        df_test[col+'_int'] = encoders[col].transform(df_test[col])\n",
    "\n",
    "print(numeric_headers)\n",
    "for col in numeric_headers:\n",
    "    df_train[col] = df_train[col].astype(np.float)\n",
    "    df_test[col] = df_test[col].astype(np.float)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    df_train[col] = ss.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "    df_test[col] = ss.transform(df_test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      -1.435513\n",
      "1       1.640729\n",
      "2       0.871668\n",
      "3       0.871668\n",
      "4      -0.666453\n",
      "5       0.871668\n",
      "6      -1.435513\n",
      "7       0.102608\n",
      "9      -0.666453\n",
      "10     -0.666453\n",
      "11     -1.435513\n",
      "12     -0.666453\n",
      "13      0.102608\n",
      "14     -1.435513\n",
      "15      1.640729\n",
      "16      0.102608\n",
      "17     -0.666453\n",
      "18      1.640729\n",
      "19      0.102608\n",
      "20      0.871668\n",
      "21     -0.666453\n",
      "23     -0.666453\n",
      "24      1.640729\n",
      "25     -1.435513\n",
      "26      0.871668\n",
      "28     -1.435513\n",
      "29     -0.666453\n",
      "30      0.871668\n",
      "31     -0.666453\n",
      "32     -1.435513\n",
      "          ...   \n",
      "971    -0.666453\n",
      "972     0.871668\n",
      "973     0.871668\n",
      "974    -1.435513\n",
      "976     0.102608\n",
      "977    -0.666453\n",
      "978     0.102608\n",
      "979     1.640729\n",
      "981    -0.666453\n",
      "982     0.102608\n",
      "983     0.102608\n",
      "984     0.871668\n",
      "985     0.102608\n",
      "986    -1.435513\n",
      "989    -1.435513\n",
      "990     0.871668\n",
      "991     0.102608\n",
      "992    -1.435513\n",
      "994     0.102608\n",
      "995    -0.666453\n",
      "998    -0.666453\n",
      "1000    0.871668\n",
      "1001   -0.666453\n",
      "1002    0.102608\n",
      "1003    0.871668\n",
      "1004    0.871668\n",
      "1005    0.102608\n",
      "1007   -0.666453\n",
      "1008    0.102608\n",
      "1009   -1.435513\n",
      "Name: Spendingongadgets, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_train['Spendingongadgets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "![tf](https://wiki.tum.de/download/attachments/25009442/tensor-flow_opengraph_h.png?version=1&modificationDate=1485888308193&api=v2)\n",
    "\n",
    "# Starting Tensorflow\n",
    "Now that we have processed the data, let's grab numpy matrices of the features we would like to predict with. In particular, we will convert everything to 32 bits (as a lot of Tensorflow is written with this arch in mind). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Smoking_int', 'Alcohol_int', 'Gender_int', 'Leftrighthanded_int', 'Education_int', 'Villagetown_int', 'Houseblockofflats_int', 'Music', 'Dance', 'Folk', 'Country', 'Classicalmusic', 'Musical', 'Pop', 'Rock', 'MetalorHardrock', 'Punk', 'HiphopRap', 'ReggaeSka', 'SwingJazz', 'Rocknroll', 'Alternative', 'Latino', 'TechnoTrance', 'Opera', 'Movies', 'Horror', 'Thriller', 'Comedy', 'Romantic', 'Scifi', 'War', 'Fantasy', 'Animated', 'Documentary', 'Western', 'Action', 'History', 'Psychology', 'Politics', 'Mathematics', 'Physics', 'Internet', 'PC', 'EconomyManagement', 'Biology', 'Chemistry', 'Reading', 'Geography', 'Foreignlanguages', 'Medicine', 'Law', 'Cars', 'Artexhibitions', 'Religion', 'outdoors', 'Dancing', 'Musicalinstruments', 'Writing', 'Passivesport', 'Activesport', 'Gardening', 'Celebrities', 'Shopping', 'Scienceandtechnology', 'Theatre', 'Funwithfriends', 'Adrenalinesports', 'Pets', 'Healthyeating', 'Age', 'Height', 'Weight', 'Numberofsiblings', 'Spendingongadgets']\n"
     ]
    }
   ],
   "source": [
    "# let's start as simply as possible, without any feature preprocessing\n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "\n",
    "# we will forego one-hot encoding right now and instead just scale all inputs\n",
    "feature_columns = categorical_headers_ints+numeric_headers\n",
    "#X_train =  ss.fit_transform(df_train[feature_columns].values).astype(np.float32)\n",
    "X =  ss.transform(df_test[feature_columns].values).astype(np.float32)\n",
    "y = df_train['Spendingongadgets'].values.astype(np.int)\n",
    "\n",
    "\n",
    "print(feature_columns)\n",
    "# split into more X_train X_test y_train y_test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=3, random_state=None, test_size=0.2,\n",
      "            train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = StratifiedShuffleSplit(n_splits=num_cv_iterations,test_size = 0.2)\n",
    "\n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "\n",
    "            X_train = (X[train_indices])\n",
    "            y_train = y[train_indices]\n",
    "\n",
    "            X_test = (X[test_indices])\n",
    "            y_test = y[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the tensorflow contrib libraries. These libraries are the workhorses for tensorflow's simplified interface. Please note that this was previously called \"SKFlow\" and might one day be replaced by the simplified syntax of Keras. Keras will received native TF support and be included in tf.contrib but as of the creation of this notebook, that is not the case. As such, we will be using syntax for the contrib library as of Rev 1.0 of tensorflow. \n",
    "\n",
    "- Please also note that keras is getting added to the core tensorflow contribution library such that this command will work:\n",
    " - `from tensorflow.contrib import keras`\n",
    "- As such, if you want to use the Keras API, it will soon be supported! Even so, its unclear what syntax changes and additions will be made to the existing Keras tools, so we will forge ahead using the \"learn\" API\n",
    "\n",
    "When using the learn API, we will still be using many of the native tensorflow functions, so we \n",
    "- `import tensorflow as tf` to get access to the entire API when needed\n",
    "- `from tensorflow.contrib import learn` to get access to many of the wrappers and simplifications for using tensorflow. This library really helps with creating models and using some out-of-the-box networks (like a shallow/deep MLP with fully connected layers)\n",
    "- `from tensorflow.contrib import layers` will be used to get access to some common neural network layer types and activations \n",
    "- `from tensorflow.contrib.learn.python import SKCompat` this will be used to give us a very familir interface as used in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.learn.python import SKCompat\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "tf.logging.set_verbosity(tf.logging.WARN) # control the verbosity of tensor flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tflearn](http://www.kdnuggets.com/wp-content/uploads/skflow.jpg)\n",
    "\n",
    "## An example similar to Sklearn\n",
    "We will start with creating a model that is similar to what we have seen in scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/9q/yt4vs1hd735gdq1f_yrg4kb40000gp/T/tmpi2gwx4hr\n",
      "WARNING:tensorflow:From /Users/vantran/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "CPU times: user 5.76 s, sys: 101 ms, total: 5.86 s\n",
      "Wall time: 5.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we need to tell tensorflow how many inputs to expect and what the data types will be\n",
    "# for this early example, everything is just numeric, real valued\n",
    "features_tf = [layers.real_valued_column('', dimension=X_train.shape[1])]\n",
    "clf = SKCompat(# wrap with SKCompat for easy usage like sklearn\n",
    "            learn.DNNClassifier(hidden_units=[50], feature_columns=features_tf)\n",
    "        )\n",
    "\n",
    "clf.fit(X_train,y_train,steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  27   0]\n",
      " [  0 106   0]\n",
      " [  0  22   0]] 0.683870967742\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "\n",
    "yhat = clf.predict(X_test)\n",
    "# notice that the output needs some interpretation\n",
    "# as its not completely the same as sklearn\n",
    "yhat = yhat['classes']\n",
    "print(mt.confusion_matrix(y_test,yhat),\n",
    "      mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/9q/yt4vs1hd735gdq1f_yrg4kb40000gp/T/tmp84ms1m5q\n",
      "WARNING:tensorflow:From /Users/vantran/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "[[  0  27   0]\n",
      " [  0 106   0]\n",
      " [  0  10  12]] 0.761290322581\n",
      "CPU times: user 7.68 s, sys: 406 ms, total: 8.09 s\n",
      "Wall time: 7.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we can also custimize the classifier somewhat:\n",
    "clf = SKCompat(# wrap with SKCompat for easy usage like sklearn\n",
    "            learn.DNNClassifier(hidden_units=[50], \n",
    "                                feature_columns=features_tf,\n",
    "                                activation_fn=tf.nn.sigmoid \n",
    "                                # tf.tanh, tf.sigmoid, tf.nn.relu, tf.nn.softmax etc.\n",
    "                                )\n",
    "        )\n",
    "\n",
    "clf.fit(X_train,y_train,steps=1000)\n",
    "\n",
    "yhat = clf.predict(X_test)['classes']\n",
    "print(mt.confusion_matrix(y_test,yhat),\n",
    "      mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Adding more customization\n",
    "This type of architecture is fine to use, but suffers from many limitations: if its not implemented by the `learn` API, then we can't use the architecture or customize it easily. To get around this, we, instead, will use the `Estimator` class. To use this class, we need to define a \"model function\" that creates the neural network architecture. The output of this model needs to be very specific: a three tuple of \n",
    "- (1) predictions in a format that the end user will need to use\n",
    "- (2) loss function implemented as tf graph \n",
    "- (3)  the optimization operation to perform on the graph. \n",
    "\n",
    "For this particular optimization, let's go beyond SGD and use Adagrad. There are many excellent explanations of different optimizers, for instance:\n",
    "- http://sebastianruder.com/optimizing-gradient-descent/\n",
    "\n",
    "There is a lot of code here that might be new, but essentially we are starting to get down to the core functionality of tensorflow (even though still using the learn API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# let's start by just using the tflearn library out of the box on the data we have\n",
    "def my_model(features, targets, mode):\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    # =====SETUP ARCHITECTURE=====\n",
    "    # we can use functions from learn to add layers and complexity to the model\n",
    "    # pass features through one hidden layer with relu activation\n",
    "    features = layers.relu(features, num_outputs=50) \n",
    "    # now pass the features through a fully connected layer\n",
    "    features = layers.fully_connected(features, num_outputs=1) \n",
    "    # and pass them through a sigmoid activation\n",
    "    output_layer = tf.sigmoid(features) \n",
    "    # reshape the output to be one dimensional\n",
    "    predictions = tf.reshape(output_layer, [-1])\n",
    "    \n",
    "    # depending on the mode, we may not want to evaluate these\n",
    "    loss_mse = None\n",
    "    train_op = None\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        # =====LOSS=======\n",
    "        # we want to use MSE as our loss function, but could also choose \n",
    "        # cross entropy, or other objective functions here\n",
    "        loss_mse = tf.losses.mean_squared_error(targets, predictions) \n",
    "    \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        # =====OPTIMIZER PARAMS========\n",
    "        # now let's setup how we want thing to optimize \n",
    "        train_op = layers.optimize_loss(\n",
    "            loss=loss_mse, \n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            optimizer='Adagrad', # adaptive gradient, so that the learning rate is not SO important\n",
    "            learning_rate=0.1)\n",
    "    \n",
    "    # what format to have the output in when calling clf.predict?\n",
    "    predictions_out = predictions>0.5\n",
    "    return model_fn_lib.ModelFnOps(\n",
    "      mode=mode, predictions=predictions_out, loss=loss_mse, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the model, its as simple as using it in a very familiar syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/9q/yt4vs1hd735gdq1f_yrg4kb40000gp/T/tmpozfuidgw\n",
      "[[  0  27   0]\n",
      " [  0 106   0]\n",
      " [  0  22   0]] 0.683870967742\n",
      "75 155\n",
      "CPU times: user 10.3 s, sys: 1.7 s, total: 12 s\n",
      "Wall time: 8.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# now let train the estimator\n",
    "# we can wrap the tf estimator in the SKCompat class\n",
    "# so that we can use similar syntax to to SKLearn \n",
    "clf = SKCompat(learn.Estimator(model_fn=my_model))\n",
    "clf.fit(X_train, y_train, steps=5000, batch_size=32)\n",
    "\n",
    "yhat = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test,yhat), \n",
    "      accuracy_score(y_test,yhat))\n",
    "\n",
    "print(X_test.shape[1], len(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Note [start review here]\n",
    "\n",
    "## Handling different feature types\n",
    "\n",
    "You may have noticed that we were not handling the categorical features correctly. Really we need to one hot encode the categorical features\n",
    "unfortunately, this requires using part of the API that breaks some conventions of SKLearn. In this example, we will use the integer categorical labels to get one hot encoded examples.\n",
    "\n",
    "The advantage of this method is that we can write our custom preprocessing steps and use them straight in the pipeline of the classifier. However, the syntax for this is verbose and fairly ugly. So, let's get started!\n",
    "\n",
    "First, we need a process input function that performs all the tensoflow preparation of the data and returns a dictionary of the name and values of each feature column. It should also return the target column as a tf.constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's start with the TF example (manipulated to work with new syntax)\n",
    "# https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "def process_input(df, label_header, categ_headers, numeric_headers):\n",
    "    # input: what ever you need it to be\n",
    "    # output: (dict of feature columns as tensors), (labels as tensors)\n",
    "    \n",
    "    # ========Process Inputs=========\n",
    "    # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "    # the values of that column stored in a constant Tensor.\n",
    "    continuous_cols = {k: tf.expand_dims( # make it a column vector\n",
    "                            tf.cast( # cast to a float32\n",
    "                                tf.constant(df[k].values), \n",
    "                                tf.float32), \n",
    "                            1)\n",
    "                       for k in numeric_headers}\n",
    "    \n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored as constant Tensors (numeric)\n",
    "    # then use tensor flow to one hot encode them using the given number of classes \n",
    "    # name of encoder is **_int need to map only to **\n",
    "    categorical_cols = {k: tf.one_hot(indices=tf.constant(df[k].values),\n",
    "                                      depth=len(encoders[k[:-4]].classes_)) \n",
    "                        for k in categ_headers}\n",
    "    \n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(continuous_cols)\n",
    "    feature_cols.update(categorical_cols)\n",
    "    \n",
    "    # Convert the label column into a constant Tensor.\n",
    "    label = None\n",
    "    if label_header is not None:\n",
    "        label = tf.constant(df[label_header].values)\n",
    "        \n",
    "    return feature_cols, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we want to use the classifier, we tell the model how it needs to parse the inputs. For each numeric feature, we simply need to convert the tensors into column vectors (see below). For the categrocial vectors, TF has a convenient \"one_hot\" function that we can use. The remainder of the model selection stays the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def my_model(dict_features, targets, mode):\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    #=======DECODE FEATURES================\n",
    "    # now let's combine the tensors from the input dictionary\n",
    "    # into a list of the feature columns\n",
    "    features = []\n",
    "    #features = [dict_features[x] for x in numeric_headers+categorical_headers_ints]\n",
    "    for col in numeric_headers:\n",
    "        features.append(dict_features[col])\n",
    "    \n",
    "    # also add in the one hot encoded features\n",
    "    for col in categorical_headers_ints:\n",
    "        features.append(dict_features[col])\n",
    "    \n",
    "    # now we can just combine all the features together\n",
    "    features = tf.concat(values=features,axis=1)\n",
    "    \n",
    "    # =====SETUP ARCHITECTURE=====\n",
    "    # we can use functions from learn to add layers and complexity to the model\n",
    "    # pass features through one hidden layer with relu activation\n",
    "    features = layers.relu(features, num_outputs=50) \n",
    "    # now pass the features through a fully connected layer\n",
    "    features = layers.fully_connected(features, num_outputs=1) \n",
    "    # and pass them through a sigmoid activation\n",
    "    output_layer = tf.sigmoid(features) \n",
    "    # reshape the output to be one dimensional\n",
    "    predictions = tf.reshape(output_layer, [-1])\n",
    "    \n",
    "    # depending on the mode, we may not want to evaluate these\n",
    "    loss_mse = None\n",
    "    train_op = None\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        # =====LOSS=======\n",
    "        # we want to use MSE as our loss function\n",
    "        loss_mse = tf.losses.mean_squared_error(targets, predictions) \n",
    "    \n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        # =====OPTIMIZER PARAMS========\n",
    "        # now let's setup how we want thing to optimize \n",
    "        train_op = layers.optimize_loss(\n",
    "            loss=loss_mse, \n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            optimizer='Adagrad', # adaptive gradient, so that the learning rate is not SO important \n",
    "            learning_rate=0.1)\n",
    "    \n",
    "    # what format to have the output in when calling clf.predict?\n",
    "    predictions_out = predictions>0.5\n",
    "    \n",
    "    return model_fn_lib.ModelFnOps(\n",
    "      mode=mode, predictions={'Spendingongadgets':predictions_out}, loss=loss_mse, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to call the estimator with our custom pre-processing step we need to wrap the preprocessing inside another function. This is easy to do with a simple lambda, but you could also use another wrapper function that takes no inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/9q/yt4vs1hd735gdq1f_yrg4kb40000gp/T/tmp59ds24m4\n",
      "CPU times: user 6.21 s, sys: 258 ms, total: 6.47 s\n",
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = learn.Estimator(model_fn=my_model)\n",
    "\n",
    "# when we provide the process function, they expect us to control the mini-batch\n",
    "clf.fit(input_fn=\n",
    "        lambda:process_input(df_train,'Spendingongadgets',categorical_headers_ints, numeric_headers), \n",
    "        steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vantran/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ebddc19a3de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m yhat = clf.predict(input_fn=\n\u001b[0;32m----> 2\u001b[0;31m                    lambda:process_input(X_test,None,categorical_headers_ints, numeric_headers))\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# the output is now an iterable value, so we need to step over it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Spendingongadgets'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m print(confusion_matrix(y_test,yhat),\n",
      "\u001b[0;32m/Users/vantran/anaconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             func.__module__, arg_name, date, instructions)\n\u001b[0;32m--> 280\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    282\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32m/Users/vantran/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, input_fn, batch_size, outputs, as_iterable)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mfeed_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         as_iterable=as_iterable)\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_variable_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vantran/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_infer_model\u001b[0;34m(self, input_fn, feed_fn, outputs, as_iterable, iterate_batches)\u001b[0m\n\u001b[1;32m    862\u001b[0m       \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_random_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m       \u001b[0mcontrib_framework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m       \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features_from_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0minfer_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_legacy_get_predict_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vantran/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_get_features_from_input_fn\u001b[0;34m(self, input_fn)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_features_from_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-ebddc19a3de2>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m yhat = clf.predict(input_fn=\n\u001b[0;32m----> 2\u001b[0;31m                    lambda:process_input(X_test,None,categorical_headers_ints, numeric_headers))\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# the output is now an iterable value, so we need to step over it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Spendingongadgets'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m print(confusion_matrix(y_test,yhat),\n",
      "\u001b[0;32m<ipython-input-18-ec85a766728b>\u001b[0m in \u001b[0;36mprocess_input\u001b[0;34m(df, label_header, categ_headers, numeric_headers)\u001b[0m\n\u001b[1;32m     13\u001b[0m                                 tf.float32), \n\u001b[1;32m     14\u001b[0m                             1)\n\u001b[0;32m---> 15\u001b[0;31m                        for k in numeric_headers}\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Creates a dictionary mapping from each categorical feature column name (k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-ec85a766728b>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m                                 tf.float32), \n\u001b[1;32m     14\u001b[0m                             1)\n\u001b[0;32m---> 15\u001b[0;31m                        for k in numeric_headers}\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Creates a dictionary mapping from each categorical feature column name (k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "yhat = clf.predict(input_fn=\n",
    "                   lambda:process_input(X_test,None,categorical_headers_ints, numeric_headers))\n",
    "# the output is now an iterable value, so we need to step over it\n",
    "yhat = [x['Spendingongadgets'] for x in yhat]\n",
    "print(confusion_matrix(y_test,yhat),\n",
    "      accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aucs = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "ind_yhat = yhat\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "all_tpr = []\n",
    "perclass_mean_tpr = 0.0\n",
    "roc_auc = 0\n",
    "classes = np.unique(y_train)\n",
    "    # get the mean fpr and tpr, per class\n",
    "for j in classes:\n",
    "    fpr, tpr, thresholds = roc_curve(y_test,\n",
    "                                     yhat,\n",
    "                                     pos_label=j)\n",
    "    perclass_mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    perclass_mean_tpr[0] = 0.0\n",
    "    roc_auc += auc(fpr, tpr)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    aucs.append(auc(fpr, tpr))\n",
    "plt.figure()\n",
    "plt.title(\"Performance of our standard MLP\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "for i in range(0,3):\n",
    "    plt.plot(fprs[i],tprs[i],label='Class %d (area = %0.2f)'\n",
    "               % (i+1, aucs[i]))\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the confusion matrix is doing pretty well! But we still are just using an MLP with one hidden layer. We really want to take advantage of the crossed columns and embeddings that are possible with tensorflow. That takes yet another syntax change in how we create out tensorflow object. \n",
    "\n",
    "### Note: ended wide/deep lecture here. \n",
    "\n",
    "## [Back to Slides]\n",
    "\n",
    "![asdfasfd](https://www.tensorflow.org/images/wide_n_deep.svg)\n",
    "____\n",
    "# Adding Crossed Columns\n",
    "For this example, we are going to combine our `learn` syntax with the syntax we have just gone over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets create a wide model \n",
    "# https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "def process_input(df, label_header, categ_headers, numeric_headers):\n",
    "    # input: what ever you need it to be\n",
    "    # output: (dict of feature columns as tensors), (labels as tensors)\n",
    "    \n",
    "    # ========Process Inputs=========\n",
    "    # not much changes here, except we leave the numerics as tc.constants\n",
    "    continuous_cols = {k: tf.constant(df[k].values) for k in numeric_headers}\n",
    "      \n",
    "    # and we shift these tensors to be sparse one-hot encoded values\n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "                              indices=[[i, 0] for i in range(df[k].size)],\n",
    "                              values=df[k].values,\n",
    "                              dense_shape=[df[k].size, 1])\n",
    "                        for k in categ_headers}\n",
    "    \n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(categorical_cols)\n",
    "    feature_cols.update(continuous_cols)\n",
    "    \n",
    "    # Convert the label column into a constant Tensor.\n",
    "    label = None\n",
    "    if label_header is not None:\n",
    "        label = tf.constant(df[label_header].values)\n",
    "        \n",
    "    return feature_cols, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_wide_columns():\n",
    "    # let's create the column structure that the learn API can expect\n",
    "    \n",
    "    wide_columns = []\n",
    "    # add in each of the categorical columns\n",
    "    for col in categorical_headers:\n",
    "        wide_columns.append(layers.sparse_column_with_keys(col, keys=encoders[col].classes_))\n",
    "        \n",
    "    # also add in some specific crossed columns\n",
    "    cross_columns = [('Smoking','Alcohol')]\n",
    "    for tup in cross_columns:\n",
    "        wide_columns.append(\n",
    "            layers.crossed_column(\n",
    "                [layers.sparse_column_with_keys(tup[0], keys=encoders[tup[0]].classes_),\n",
    "                 layers.sparse_column_with_keys(tup[1], keys=encoders[tup[1]].classes_)],\n",
    "            hash_bucket_size=int(1e4))\n",
    "        )\n",
    "                        \n",
    "    return wide_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(categorical_headers)\n",
    "print(numeric_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# ignore all the deprecations that the learn API needs to deal with... ugh\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# setup \n",
    "wide_columns = setup_wide_columns()\n",
    "input_wrapper = lambda:process_input(df_train,'Spendingongadgets',categorical_headers, numeric_headers)\n",
    "output_wrapper = lambda:process_input(df_test,None,categorical_headers, numeric_headers)\n",
    "\n",
    "clf = learn.LinearClassifier(feature_columns=wide_columns)\n",
    "\n",
    "# when we provide the process function, they expect us to control the mini-batch\n",
    "clf.fit(input_fn=input_wrapper, steps=300)\n",
    "\n",
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "# the output is now an iterable value, so we need to step over it\n",
    "yhat = [x for x in yhat]\n",
    "print(confusion_matrix(y_test,yhat),accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That is just using crossed columns and a one layer Linear Classifer! So memorization works fairly well here. \n",
    "\n",
    "Now let's try a deeper architecture with dense embeddings for the categorical features, as described in lecture.\n",
    "\n",
    "___\n",
    "\n",
    "## Using Dense embeddings in a deeper network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_deep_columns():\n",
    "    # now make up the deep columns\n",
    "    \n",
    "    deep_columns = []\n",
    "    # add in each of the categorical columns to both wide and deep features\n",
    "    for col in categorical_headers:\n",
    "        \n",
    "        tmp = layers.sparse_column_with_keys(col, keys=encoders[col].classes_)\n",
    "        \n",
    "        deep_columns.append(\n",
    "            layers.embedding_column(tmp, dimension=8)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # and add in the regular dense features \n",
    "    for col in numeric_headers:\n",
    "        deep_columns.append(\n",
    "            layers.real_valued_column(col)\n",
    "        )\n",
    "                    \n",
    "    return deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# setup deep columns\n",
    "deep_columns = setup_deep_columns()\n",
    "clf = learn.DNNClassifier(feature_columns=deep_columns, hidden_units=[100, 50])\n",
    "\n",
    "clf.fit(input_fn=input_wrapper, steps=300)\n",
    "\n",
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "# the output is now an iterable value, so we need to step over it\n",
    "yhat = [x for x in yhat]\n",
    "print(confusion_matrix(y_test,yhat),\n",
    "      accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the DNN with feature embeddings is also fairly capable (best thus far). For the final example, lets combine the classifiers together like we talked about in lecture.\n",
    "___\n",
    "\n",
    "## Combining Crossed Linear Classifier and Deep Embeddings\n",
    "Now its just a matter of setting the wide and deep columns for tensorflow. After which, we can use the combined classifier which is already implemented! `learn.DNNLinearCombinedClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_wide_deep_columns():\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    wide_columns = []\n",
    "    deep_columns = []\n",
    "    # add in each of the categorical columns to both wide and deep features\n",
    "    for col in categorical_headers:\n",
    "        wide_columns.append(\n",
    "            layers.sparse_column_with_keys(col, keys=encoders[col].classes_)\n",
    "        )\n",
    "        \n",
    "        dim = round(np.log2(len(encoders[col].classes_)))\n",
    "        deep_columns.append(\n",
    "            layers.embedding_column(wide_columns[-1], dimension=dim)\n",
    "        )\n",
    "        \n",
    "    # also add in some specific crossed columns\n",
    "    cross_columns = [('Smoking','Alcohol')]\n",
    "    for tup in cross_columns:\n",
    "        wide_columns.append(\n",
    "            layers.crossed_column(\n",
    "                [layers.sparse_column_with_keys(tup[0], keys=encoders[tup[0]].classes_),\n",
    "                 layers.sparse_column_with_keys(tup[1], keys=encoders[tup[1]].classes_)],\n",
    "            hash_bucket_size=int(1e4))\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # and add in the regular dense features \n",
    "    for col in numeric_headers:\n",
    "        deep_columns.append(\n",
    "            layers.real_valued_column(col)\n",
    "        )\n",
    "                    \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wide_columns, deep_columns = setup_wide_deep_columns()\n",
    "clf = learn.DNNLinearCombinedClassifier(\n",
    "                        linear_feature_columns=wide_columns,\n",
    "                        dnn_feature_columns=deep_columns,\n",
    "                        dnn_hidden_units=[100, 50])\n",
    "\n",
    "\n",
    "clf.fit(input_fn=input_wrapper, steps=2500)\n",
    "\n",
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "# the output is now an iterable value, so we need to step over it\n",
    "yhat = [x for x in yhat]\n",
    "print(confusion_matrix(y_test,yhat),accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aucs = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "ind_yhat = yhat\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "all_tpr = []\n",
    "perclass_mean_tpr = 0.0\n",
    "roc_auc = 0\n",
    "classes = np.unique(y_train)\n",
    "    # get the mean fpr and tpr, per class\n",
    "for j in classes:\n",
    "    fpr, tpr, thresholds = roc_curve(y_test,\n",
    "                                     yhat,\n",
    "                                     pos_label=j)\n",
    "    perclass_mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    perclass_mean_tpr[0] = 0.0\n",
    "    roc_auc += auc(fpr, tpr)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    aucs.append(auc(fpr, tpr))\n",
    "plt.figure()\n",
    "plt.title(\"Performance of our best wide and deep network\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "for i in range(0,3):\n",
    "    plt.plot(fprs[i],tprs[i],label='Class %d (area = %0.2f)'\n",
    "               % (i+1, aucs[i]))\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
