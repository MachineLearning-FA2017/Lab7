{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UCI](http://mlr.cs.umass.edu/ml/assets/logo.gif)\n",
    "\n",
    "# Loading in the Data\n",
    "In this example, we are going to use crossed columns and embedding columns inside of a tensorflow object created with the contrib \"learn\" library.\n",
    "\n",
    "However, we will start the process by loading up a dataset with a mix of categorical data and numeric data. This dataset is quite old and has been used many times in machine learning examples: the census data from 1990's. We will use it to predict if a person will earn over or under 50k per year.\n",
    "\n",
    "- https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Music</th>\n",
       "      <th>Dance</th>\n",
       "      <th>Folk</th>\n",
       "      <th>Country</th>\n",
       "      <th>Classical music</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Pop</th>\n",
       "      <th>Rock</th>\n",
       "      <th>Metal or Hardrock</th>\n",
       "      <th>Punk</th>\n",
       "      <th>...</th>\n",
       "      <th>Spending on gadgets</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Number of siblings</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Left - right handed</th>\n",
       "      <th>Education</th>\n",
       "      <th>Village - town</th>\n",
       "      <th>House - block of flats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>village</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>left handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>primary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>left handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>left handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>left handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>male</td>\n",
       "      <td>left handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>left handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>masters degree</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>masters degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>NaN</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>NaN</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>masters degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>16.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>primary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>primary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>male</td>\n",
       "      <td>left handed</td>\n",
       "      <td>masters degree</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Music  Dance  Folk  Country  Classical music  Musical  Pop  Rock  \\\n",
       "0       5.0    2.0   1.0      2.0              2.0      1.0  5.0   5.0   \n",
       "1       4.0    2.0   1.0      1.0              1.0      2.0  3.0   5.0   \n",
       "2       5.0    2.0   2.0      3.0              4.0      5.0  3.0   5.0   \n",
       "3       5.0    2.0   1.0      1.0              1.0      1.0  2.0   2.0   \n",
       "4       5.0    4.0   3.0      2.0              4.0      3.0  5.0   3.0   \n",
       "5       5.0    2.0   3.0      2.0              3.0      3.0  2.0   5.0   \n",
       "6       5.0    5.0   3.0      1.0              2.0      2.0  5.0   3.0   \n",
       "7       5.0    3.0   2.0      1.0              2.0      2.0  4.0   5.0   \n",
       "8       5.0    3.0   1.0      1.0              2.0      4.0  3.0   5.0   \n",
       "9       5.0    2.0   5.0      2.0              2.0      5.0  3.0   5.0   \n",
       "10      5.0    3.0   2.0      1.0              2.0      3.0  4.0   3.0   \n",
       "11      5.0    1.0   1.0      1.0              4.0      1.0  2.0   5.0   \n",
       "12      5.0    1.0   2.0      1.0              4.0      3.0  3.0   5.0   \n",
       "13      5.0    5.0   3.0      2.0              1.0      5.0  5.0   2.0   \n",
       "14      5.0    2.0   1.0      1.0              2.0      3.0  4.0   5.0   \n",
       "15      1.0    2.0   2.0      3.0              4.0      3.0  3.0   5.0   \n",
       "16      5.0    3.0   1.0      1.0              1.0      2.0  4.0   4.0   \n",
       "17      5.0    3.0   3.0      3.0              2.0      2.0  4.0   4.0   \n",
       "18      5.0    5.0   4.0      3.0              4.0      5.0  5.0   4.0   \n",
       "19      5.0    3.0   3.0      2.0              4.0      2.0  2.0   4.0   \n",
       "20      5.0    3.0   2.0      3.0              4.0      3.0  2.0   5.0   \n",
       "21      5.0    1.0   1.0      3.0              2.0      2.0  2.0   5.0   \n",
       "22      5.0    3.0   2.0      3.0              3.0      3.0  4.0   NaN   \n",
       "23      5.0    4.0   2.0      2.0              2.0      4.0  4.0   5.0   \n",
       "24      5.0    3.0   1.0      1.0              4.0      3.0  3.0   5.0   \n",
       "25      5.0    4.0   2.0      1.0              2.0      3.0  5.0   1.0   \n",
       "26      5.0    5.0   5.0      4.0              5.0      3.0  4.0   4.0   \n",
       "27      4.0    3.0   4.0      1.0              3.0      2.0  2.0   4.0   \n",
       "28      5.0    5.0   1.0      1.0              1.0      1.0  3.0   4.0   \n",
       "29      5.0    3.0   4.0      2.0              3.0      3.0  3.0   4.0   \n",
       "...     ...    ...   ...      ...              ...      ...  ...   ...   \n",
       "980     5.0    2.0   2.0      2.0              5.0      1.0  4.0   4.0   \n",
       "981     5.0    1.0   2.0      4.0              5.0      5.0  3.0   4.0   \n",
       "982     5.0    2.0   1.0      1.0              3.0      2.0  2.0   4.0   \n",
       "983     5.0    3.0   1.0      1.0              3.0      3.0  5.0   3.0   \n",
       "984     5.0    1.0   3.0      2.0              3.0      3.0  3.0   4.0   \n",
       "985     5.0    1.0   3.0      2.0              4.0      2.0  4.0   5.0   \n",
       "986     4.0    2.0   5.0      3.0              5.0      2.0  2.0   4.0   \n",
       "987     4.0    2.0   2.0      3.0              2.0      1.0  1.0   5.0   \n",
       "988     5.0    3.0   2.0      2.0              5.0      2.0  2.0   5.0   \n",
       "989     5.0    3.0   2.0      2.0              4.0      5.0  5.0   5.0   \n",
       "990     5.0    2.0   1.0      1.0              2.0      2.0  3.0   4.0   \n",
       "991     5.0    2.0   1.0      1.0              1.0      1.0  1.0   5.0   \n",
       "992     4.0    4.0   1.0      4.0              4.0      1.0  3.0   4.0   \n",
       "993     5.0    3.0   1.0      2.0              3.0      1.0  4.0   4.0   \n",
       "994     5.0    3.0   2.0      2.0              3.0      4.0  3.0   5.0   \n",
       "995     5.0    2.0   1.0      1.0              1.0      2.0  1.0   4.0   \n",
       "996     5.0    1.0   1.0      1.0              3.0      2.0  1.0   4.0   \n",
       "997     5.0    3.0   3.0      3.0              1.0      3.0  3.0   4.0   \n",
       "998     5.0    4.0   3.0      1.0              3.0      2.0  3.0   5.0   \n",
       "999     5.0    5.0   3.0      4.0              5.0      4.0  4.0   4.0   \n",
       "1000    5.0    3.0   3.0      2.0              3.0      3.0  4.0   3.0   \n",
       "1001    5.0    5.0   1.0      2.0              1.0      4.0  4.0   3.0   \n",
       "1002    5.0    3.0   1.0      3.0              1.0      1.0  2.0   5.0   \n",
       "1003    4.0    4.0   3.0      2.0              2.0      3.0  4.0   5.0   \n",
       "1004    5.0    4.0   1.0      2.0              3.0      2.0  3.0   3.0   \n",
       "1005    5.0    5.0   2.0      2.0              5.0      4.0  4.0   4.0   \n",
       "1006    4.0    5.0   1.0      3.0              4.0      1.0  4.0   1.0   \n",
       "1007    4.0    1.0   1.0      2.0              2.0      2.0  3.0   4.0   \n",
       "1008    5.0    3.0   3.0      1.0              3.0      1.0  3.0   4.0   \n",
       "1009    5.0    4.0   3.0      2.0              3.0      3.0  4.0   1.0   \n",
       "\n",
       "      Metal or Hardrock  Punk           ...            Spending on gadgets  \\\n",
       "0                   1.0   1.0           ...                              1   \n",
       "1                   4.0   4.0           ...                              5   \n",
       "2                   3.0   4.0           ...                              4   \n",
       "3                   1.0   4.0           ...                              4   \n",
       "4                   1.0   2.0           ...                              2   \n",
       "5                   5.0   3.0           ...                              4   \n",
       "6                   1.0   1.0           ...                              1   \n",
       "7                   1.0   2.0           ...                              3   \n",
       "8                   5.0   1.0           ...                              3   \n",
       "9                   2.0   3.0           ...                              2   \n",
       "10                  2.0   1.0           ...                              2   \n",
       "11                  1.0   1.0           ...                              1   \n",
       "12                  4.0   2.0           ...                              2   \n",
       "13                  1.0   1.0           ...                              3   \n",
       "14                  2.0   5.0           ...                              1   \n",
       "15                  5.0   5.0           ...                              5   \n",
       "16                  1.0   3.0           ...                              3   \n",
       "17                  2.0   3.0           ...                              2   \n",
       "18                  4.0   3.0           ...                              5   \n",
       "19                  5.0   2.0           ...                              3   \n",
       "20                  5.0   4.0           ...                              4   \n",
       "21                  5.0   4.0           ...                              2   \n",
       "22                  1.0   2.0           ...                              4   \n",
       "23                  2.0   3.0           ...                              2   \n",
       "24                  5.0   5.0           ...                              5   \n",
       "25                  1.0   1.0           ...                              1   \n",
       "26                  3.0   2.0           ...                              4   \n",
       "27                  2.0   4.0           ...                              2   \n",
       "28                  1.0   3.0           ...                              1   \n",
       "29                  1.0   3.0           ...                              2   \n",
       "...                 ...   ...           ...                            ...   \n",
       "980                 4.0   2.0           ...                              4   \n",
       "981                 1.0   1.0           ...                              2   \n",
       "982                 3.0   2.0           ...                              3   \n",
       "983                 1.0   1.0           ...                              3   \n",
       "984                 4.0   3.0           ...                              4   \n",
       "985                 3.0   3.0           ...                              3   \n",
       "986                 2.0   4.0           ...                              1   \n",
       "987                 3.0   2.0           ...                              4   \n",
       "988                 2.0   4.0           ...                              5   \n",
       "989                 5.0   1.0           ...                              1   \n",
       "990                 4.0   1.0           ...                              4   \n",
       "991                 5.0   4.0           ...                              3   \n",
       "992                 4.0   3.0           ...                              1   \n",
       "993                 1.0   1.0           ...                              4   \n",
       "994                 3.0   4.0           ...                              3   \n",
       "995                 4.0   3.0           ...                              2   \n",
       "996                 3.0   1.0           ...                              3   \n",
       "997                 2.0   4.0           ...                              2   \n",
       "998                 5.0   4.0           ...                              2   \n",
       "999                 3.0   NaN           ...                              3   \n",
       "1000                1.0   1.0           ...                              4   \n",
       "1001                2.0   4.0           ...                              2   \n",
       "1002                5.0   3.0           ...                              3   \n",
       "1003                2.0   3.0           ...                              4   \n",
       "1004                4.0   3.0           ...                              4   \n",
       "1005                3.0   2.0           ...                              3   \n",
       "1006                1.0   4.0           ...                              5   \n",
       "1007                1.0   2.0           ...                              2   \n",
       "1008                1.0   1.0           ...                              3   \n",
       "1009                1.0   2.0           ...                              1   \n",
       "\n",
       "       Age  Height  Weight  Number of siblings  Gender  Left - right handed  \\\n",
       "0     20.0   163.0    48.0                 1.0  female         right handed   \n",
       "1     19.0   163.0    58.0                 2.0  female         right handed   \n",
       "2     20.0   176.0    67.0                 2.0  female         right handed   \n",
       "3     22.0   172.0    59.0                 1.0  female         right handed   \n",
       "4     20.0   170.0    59.0                 1.0  female         right handed   \n",
       "5     20.0   186.0    77.0                 1.0    male         right handed   \n",
       "6     20.0   177.0    50.0                 1.0  female         right handed   \n",
       "7     19.0   184.0    90.0                 1.0    male         right handed   \n",
       "8     18.0   166.0    55.0                 1.0  female         right handed   \n",
       "9     19.0   174.0    60.0                 3.0  female         right handed   \n",
       "10    19.0   175.0    60.0                 2.0  female          left handed   \n",
       "11    17.0   176.0    60.0                 1.0  female         right handed   \n",
       "12    24.0   168.0    55.0                10.0  female         right handed   \n",
       "13    19.0   165.0    55.0                 1.0  female         right handed   \n",
       "14    22.0   175.0    57.0                 1.0  female         right handed   \n",
       "15    18.0   177.0    77.0                 0.0    male         right handed   \n",
       "16    19.0   175.0    65.0                 2.0  female         right handed   \n",
       "17    20.0   168.0    65.0                 1.0  female         right handed   \n",
       "18    18.0   181.0    78.0                 2.0    male         right handed   \n",
       "19    18.0   188.0    90.0                 1.0    male          left handed   \n",
       "20    20.0   186.0    77.0                 1.0    male         right handed   \n",
       "21    24.0   186.0    85.0                 1.0    male          left handed   \n",
       "22    22.0   167.0    70.0                 1.0  female         right handed   \n",
       "23    20.0   168.0    54.0                 3.0  female         right handed   \n",
       "24    19.0   167.0    51.0                 1.0  female         right handed   \n",
       "25    20.0   170.0    56.0                 1.0  female         right handed   \n",
       "26    22.0   165.0    63.0                 1.0  female          left handed   \n",
       "27    19.0   163.0    50.0                 1.0  female         right handed   \n",
       "28    20.0   187.0    80.0                 2.0    male          left handed   \n",
       "29    19.0   168.0    60.0                 2.0  female         right handed   \n",
       "...    ...     ...     ...                 ...     ...                  ...   \n",
       "980   18.0   166.0    53.0                 2.0  female         right handed   \n",
       "981   19.0   168.0    57.0                 1.0  female          left handed   \n",
       "982   18.0   178.0    70.0                 2.0    male         right handed   \n",
       "983   22.0   172.0    60.0                 2.0  female         right handed   \n",
       "984   21.0   165.0    55.0                 1.0  female         right handed   \n",
       "985   20.0   167.0    53.0                 1.0  female         right handed   \n",
       "986   19.0   161.0    60.0                 3.0  female         right handed   \n",
       "987   20.0   184.0    80.0                 1.0    male         right handed   \n",
       "988   19.0   183.0    70.0                 0.0    male         right handed   \n",
       "989   30.0   168.0    54.0                 2.0  female         right handed   \n",
       "990   29.0   180.0    93.0                 1.0    male         right handed   \n",
       "991   21.0   172.0    80.0                 0.0    male         right handed   \n",
       "992   30.0   200.0   150.0                 1.0    male         right handed   \n",
       "993   21.0   173.0    55.0                 0.0  female         right handed   \n",
       "994   20.0   176.0    78.0                 0.0  female         right handed   \n",
       "995   18.0   163.0    53.0                 0.0  female         right handed   \n",
       "996   20.0   190.0    74.0                 0.0    male         right handed   \n",
       "997   19.0   167.0    53.0                 1.0  female         right handed   \n",
       "998   28.0   192.0   105.0                 1.0    male         right handed   \n",
       "999   19.0   181.0    70.0                 1.0    male         right handed   \n",
       "1000  16.0   153.0    62.0                 1.0  female         right handed   \n",
       "1001  18.0   160.0    63.0                 2.0  female         right handed   \n",
       "1002  22.0   181.0    72.0                 1.0    male         right handed   \n",
       "1003  20.0   172.0    63.0                 1.0  female         right handed   \n",
       "1004  22.0   175.0    60.0                 1.0    male         right handed   \n",
       "1005  20.0   164.0    57.0                 1.0  female         right handed   \n",
       "1006  27.0   183.0    80.0                 5.0    male          left handed   \n",
       "1007  18.0   173.0    75.0                 0.0  female         right handed   \n",
       "1008  25.0   173.0    58.0                 1.0  female         right handed   \n",
       "1009  21.0   185.0    72.0                 1.0    male         right handed   \n",
       "\n",
       "                    Education  Village - town  House - block of flats  \n",
       "0     college/bachelor degree         village          block of flats  \n",
       "1     college/bachelor degree            city          block of flats  \n",
       "2            secondary school            city          block of flats  \n",
       "3     college/bachelor degree            city          house/bungalow  \n",
       "4            secondary school         village          house/bungalow  \n",
       "5            secondary school            city          block of flats  \n",
       "6            secondary school         village          house/bungalow  \n",
       "7     college/bachelor degree            city          house/bungalow  \n",
       "8            secondary school            city          house/bungalow  \n",
       "9            secondary school            city          block of flats  \n",
       "10           secondary school            city          block of flats  \n",
       "11             primary school            city          block of flats  \n",
       "12    college/bachelor degree            city          block of flats  \n",
       "13           secondary school            city          block of flats  \n",
       "14    college/bachelor degree            city          block of flats  \n",
       "15           secondary school            city          block of flats  \n",
       "16    college/bachelor degree            city          block of flats  \n",
       "17           secondary school         village          house/bungalow  \n",
       "18           secondary school            city          house/bungalow  \n",
       "19           secondary school            city          house/bungalow  \n",
       "20           secondary school            city          block of flats  \n",
       "21           secondary school            city          block of flats  \n",
       "22    college/bachelor degree            city          house/bungalow  \n",
       "23           secondary school            city          block of flats  \n",
       "24           secondary school            city          block of flats  \n",
       "25           secondary school            city          block of flats  \n",
       "26    college/bachelor degree            city          house/bungalow  \n",
       "27           secondary school            city          house/bungalow  \n",
       "28           secondary school         village          house/bungalow  \n",
       "29           secondary school         village          block of flats  \n",
       "...                       ...             ...                     ...  \n",
       "980          secondary school            city          house/bungalow  \n",
       "981          secondary school         village          house/bungalow  \n",
       "982          secondary school            city          house/bungalow  \n",
       "983          secondary school         village          house/bungalow  \n",
       "984          secondary school            city          block of flats  \n",
       "985          secondary school            city          block of flats  \n",
       "986          secondary school            city          house/bungalow  \n",
       "987          secondary school            city          block of flats  \n",
       "988          secondary school            city          block of flats  \n",
       "989            masters degree         village          house/bungalow  \n",
       "990          secondary school            city          block of flats  \n",
       "991          secondary school            city          house/bungalow  \n",
       "992            masters degree            city          block of flats  \n",
       "993          secondary school            city          block of flats  \n",
       "994          secondary school            city          block of flats  \n",
       "995          secondary school            city          block of flats  \n",
       "996          secondary school             NaN          block of flats  \n",
       "997          secondary school             NaN          block of flats  \n",
       "998            masters degree            city          block of flats  \n",
       "999          secondary school            city          block of flats  \n",
       "1000           primary school            city          block of flats  \n",
       "1001           primary school            city          block of flats  \n",
       "1002         secondary school            city          block of flats  \n",
       "1003         secondary school            city          house/bungalow  \n",
       "1004         secondary school            city          block of flats  \n",
       "1005         secondary school            city          house/bungalow  \n",
       "1006           masters degree         village          house/bungalow  \n",
       "1007         secondary school            city          block of flats  \n",
       "1008  college/bachelor degree            city          block of flats  \n",
       "1009         secondary school         village          house/bungalow  \n",
       "\n",
       "[1010 rows x 75 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.special import expit\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from memory_profiler import memory_usage\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "target_classifier = 'Spending on gadgets'\n",
    "df = pd.read_csv('responses.csv', sep=\",\")\n",
    "desired_features = [\"Music\",\"Dance\",\"Folk\",\"Country\",\"Classical music\",\"Musical\",\n",
    "    \"Pop\",\"Rock\",\"Metal or Hardrock\",\"Punk\",\"Hiphop, Rap\",\"Reggae, Ska\",\n",
    "    \"Swing, Jazz\",\"Rock n roll\",\"Alternative\",\"Latino\",\"Techno, Trance\",\n",
    "    \"Opera\",\"Movies\",\"Horror\",\"Thriller\",\"Comedy\",\"Romantic\",\"Sci-fi\",\"War\",\n",
    "    \"Fantasy/Fairy tales\",\"Animated\",\"Documentary\",\"Western\",\"Action\",\"History\",\n",
    "    \"Psychology\",\"Politics\",\"Mathematics\",\"Physics\",\"Internet\",\"PC\",\"Economy Management\",\n",
    "    \"Biology\",\"Chemistry\",\"Reading\",\"Geography\",\"Foreign languages\",\"Medicine\",\"Law\",\n",
    "    \"Cars\",\"Art exhibitions\",\"Religion\",\"Countryside, outdoors\",\"Dancing\",\n",
    "    \"Musical instruments\",\"Writing\",\"Passive sport\",\"Active sport\",\"Gardening\",\"Celebrities\",\n",
    "    \"Shopping\",\"Science and technology\",\"Theatre\",\"Fun with friends\",\"Adrenaline sports\",\n",
    "    \"Pets\",\"Smoking\",\"Alcohol\",\"Healthy eating\",\"Spending on gadgets\",\"Age\",\"Height\",\"Weight\",\n",
    "    \"Number of siblings\",\"Gender\",\"Left - right handed\",\"Education\",\"Village - town\",\"House - block of flats\"\n",
    "]\n",
    "\n",
    "df = df[desired_features]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'Reggae, Ska': 'ReggaeSka', 'Classical music': 'Classicalmusic', 'Metal or Hardrock': 'MetalorHardrock', 'Hiphop, Rap': 'HiphopRap', 'Rock n roll': 'Rocknroll', 'Techno, Trance': 'TechnoTrance', 'Sci-fi': 'Scifi', 'Fantasy/Fairy tales': 'Fantasy', 'Economy Management': 'EconomyManagement', 'Foreign languages': 'Foreignlanguages', 'Art exhibitions': 'Artexhibitions', 'Countryside, outdoors': 'outdoors', 'Musical instruments': 'Musicalinstruments', 'Passive sport': 'Passivesport', 'Active sport': 'Activesport', 'Science and technology': 'Scienceandtechnology', 'Fun with friends': 'Funwithfriends', 'Adrenaline sports': 'Adrenalinesports', 'Healthy eating': 'Healthyeating', 'Spending on gadgets': 'Spendingongadgets', 'Number of siblings': 'Numberofsiblings', 'Left - right handed': 'Leftrighthanded', 'Village - town': 'Villagetown', 'House - block of flats': 'Houseblockofflats'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "df_train = deepcopy(df)\n",
    "df_test = deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Music</th>\n",
       "      <th>Dance</th>\n",
       "      <th>Folk</th>\n",
       "      <th>Country</th>\n",
       "      <th>Classicalmusic</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Pop</th>\n",
       "      <th>Rock</th>\n",
       "      <th>MetalorHardrock</th>\n",
       "      <th>Punk</th>\n",
       "      <th>...</th>\n",
       "      <th>Spendingongadgets</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Numberofsiblings</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Leftrighthanded</th>\n",
       "      <th>Education</th>\n",
       "      <th>Villagetown</th>\n",
       "      <th>Houseblockofflats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>village</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>city</td>\n",
       "      <td>block of flats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>college/bachelor degree</td>\n",
       "      <td>city</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>right handed</td>\n",
       "      <td>secondary school</td>\n",
       "      <td>village</td>\n",
       "      <td>house/bungalow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Music  Dance  Folk  Country  Classicalmusic  Musical  Pop  Rock  \\\n",
       "0    5.0    2.0   1.0      2.0             2.0      1.0  5.0   5.0   \n",
       "1    4.0    2.0   1.0      1.0             1.0      2.0  3.0   5.0   \n",
       "2    5.0    2.0   2.0      3.0             4.0      5.0  3.0   5.0   \n",
       "3    5.0    2.0   1.0      1.0             1.0      1.0  2.0   2.0   \n",
       "4    5.0    4.0   3.0      2.0             4.0      3.0  5.0   3.0   \n",
       "\n",
       "   MetalorHardrock  Punk        ...          Spendingongadgets   Age  Height  \\\n",
       "0              1.0   1.0        ...                          1  20.0   163.0   \n",
       "1              4.0   4.0        ...                          5  19.0   163.0   \n",
       "2              3.0   4.0        ...                          4  20.0   176.0   \n",
       "3              1.0   4.0        ...                          4  22.0   172.0   \n",
       "4              1.0   2.0        ...                          2  20.0   170.0   \n",
       "\n",
       "   Weight  Numberofsiblings  Gender  Leftrighthanded                Education  \\\n",
       "0    48.0               1.0  female     right handed  college/bachelor degree   \n",
       "1    58.0               2.0  female     right handed  college/bachelor degree   \n",
       "2    67.0               2.0  female     right handed         secondary school   \n",
       "3    59.0               1.0  female     right handed  college/bachelor degree   \n",
       "4    59.0               1.0  female     right handed         secondary school   \n",
       "\n",
       "   Villagetown  Houseblockofflats  \n",
       "0      village     block of flats  \n",
       "1         city     block of flats  \n",
       "2         city     block of flats  \n",
       "3         city     house/bungalow  \n",
       "4      village     house/bungalow  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# let's just get rid of rows with any missing data\n",
    "# and then reset the indices of the dataframe so it corresponds to row number\n",
    "df_train.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.reset_index()\n",
    "\n",
    "df_test.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index()\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "For preprocessing, we are going to fix a few issues in the dataset. \n",
    "\n",
    "- This first includes the use of \"50K.\" instead of \"50K\" in the test set. \n",
    "- Next, we will encode the categorical features as integers (later on we will encode one hot)\n",
    "- Finally, we will make certain all the continuous data is scaled properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_headers = list(df.select_dtypes(include=['object']).columns)\n",
    "numeric_headers = list(df.select_dtypes(include=['float']).columns) + list(df.select_dtypes(include=['int']).columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Music', 'Dance', 'Folk', 'Country', 'Classicalmusic', 'Musical', 'Pop', 'Rock', 'MetalorHardrock', 'Punk', 'HiphopRap', 'ReggaeSka', 'Swing, Jazz', 'Rocknroll', 'Alternative', 'Latino', 'TechnoTrance', 'Opera', 'Movies', 'Horror', 'Thriller', 'Comedy', 'Romantic', 'Scifi', 'War', 'Fantasy', 'Animated', 'Documentary', 'Western', 'Action', 'History', 'Psychology', 'Politics', 'Mathematics', 'Physics', 'Internet', 'PC', 'EconomyManagement', 'Biology', 'Chemistry', 'Reading', 'Geography', 'Foreignlanguages', 'Medicine', 'Law', 'Cars', 'Artexhibitions', 'Religion', 'outdoors', 'Dancing', 'Musicalinstruments', 'Writing', 'Passivesport', 'Activesport', 'Gardening', 'Celebrities', 'Shopping', 'Scienceandtechnology', 'Theatre', 'Funwithfriends', 'Adrenalinesports', 'Pets', 'Healthyeating', 'Age', 'Height', 'Weight', 'Numberofsiblings', 'Spendingongadgets']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "encoders = dict() \n",
    "\n",
    "for col in categorical_headers:\n",
    "    df_train[col] = df_train[col].str.strip()\n",
    "    df_test[col] = df_test[col].str.strip()\n",
    "    \n",
    "    if col=='Spendingongadgets':\n",
    "        tmp = LabelEncoder()\n",
    "        df_train[col] = tmp.fit_transform(df_train[col])\n",
    "        df_test[col] = tmp.transform(df_test[col])\n",
    "    else:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df_train[col+'_int'] = encoders[col].fit_transform(df_train[col])\n",
    "        df_test[col+'_int'] = encoders[col].transform(df_test[col])\n",
    "\n",
    "print(numeric_headers)\n",
    "for col in numeric_headers:\n",
    "    df_train[col] = df_train[col].astype(np.float)\n",
    "    df_test[col] = df_test[col].astype(np.float)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    df_train[col] = ss.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "    df_test[col] = ss.transform(df_test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "![tf](https://wiki.tum.de/download/attachments/25009442/tensor-flow_opengraph_h.png?version=1&modificationDate=1485888308193&api=v2)\n",
    "\n",
    "# Starting Tensorflow\n",
    "Now that we have processed the data, let's grab numpy matrices of the features we would like to predict with. In particular, we will convert everything to 32 bits (as a lot of Tensorflow is written with this arch in mind). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Smoking_int', 'Alcohol_int', 'Gender_int', 'Leftrighthanded_int', 'Education_int', 'Villagetown_int', 'Houseblockofflats_int', 'Music', 'Dance', 'Folk', 'Country', 'Classicalmusic', 'Musical', 'Pop', 'Rock', 'MetalorHardrock', 'Punk', 'HiphopRap', 'ReggaeSka', 'Swing, Jazz', 'Rocknroll', 'Alternative', 'Latino', 'TechnoTrance', 'Opera', 'Movies', 'Horror', 'Thriller', 'Comedy', 'Romantic', 'Scifi', 'War', 'Fantasy', 'Animated', 'Documentary', 'Western', 'Action', 'History', 'Psychology', 'Politics', 'Mathematics', 'Physics', 'Internet', 'PC', 'EconomyManagement', 'Biology', 'Chemistry', 'Reading', 'Geography', 'Foreignlanguages', 'Medicine', 'Law', 'Cars', 'Artexhibitions', 'Religion', 'outdoors', 'Dancing', 'Musicalinstruments', 'Writing', 'Passivesport', 'Activesport', 'Gardening', 'Celebrities', 'Shopping', 'Scienceandtechnology', 'Theatre', 'Funwithfriends', 'Adrenalinesports', 'Pets', 'Healthyeating', 'Age', 'Height', 'Weight', 'Numberofsiblings', 'Spendingongadgets']\n"
     ]
    }
   ],
   "source": [
    "# let's start as simply as possible, without any feature preprocessing\n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "\n",
    "# we will forego one-hot encoding right now and instead just scale all inputs\n",
    "feature_columns = categorical_headers_ints+numeric_headers\n",
    "X_train =  ss.fit_transform(df_train[feature_columns].values).astype(np.float32)\n",
    "X_test =  ss.transform(df_test[feature_columns].values).astype(np.float32)\n",
    "\n",
    "y_train = df_train['Spendingongadgets'].values.astype(np.int)\n",
    "y_test = df_test['Spendingongadgets'].values.astype(np.int)\n",
    "\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the tensorflow contrib libraries. These libraries are the workhorses for tensorflow's simplified interface. Please note that this was previously called \"SKFlow\" and might one day be replaced by the simplified syntax of Keras. Keras will received native TF support and be included in tf.contrib but as of the creation of this notebook, that is not the case. As such, we will be using syntax for the contrib library as of Rev 1.0 of tensorflow. \n",
    "\n",
    "- Please also note that keras is getting added to the core tensorflow contribution library such that this command will work:\n",
    " - `from tensorflow.contrib import keras`\n",
    "- As such, if you want to use the Keras API, it will soon be supported! Even so, its unclear what syntax changes and additions will be made to the existing Keras tools, so we will forge ahead using the \"learn\" API\n",
    "\n",
    "When using the learn API, we will still be using many of the native tensorflow functions, so we \n",
    "- `import tensorflow as tf` to get access to the entire API when needed\n",
    "- `from tensorflow.contrib import learn` to get access to many of the wrappers and simplifications for using tensorflow. This library really helps with creating models and using some out-of-the-box networks (like a shallow/deep MLP with fully connected layers)\n",
    "- `from tensorflow.contrib import layers` will be used to get access to some common neural network layer types and activations \n",
    "- `from tensorflow.contrib.learn.python import SKCompat` this will be used to give us a very familir interface as used in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.learn.python import SKCompat\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "tf.logging.set_verbosity(tf.logging.WARN) # control the verbosity of tensor flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tflearn](http://www.kdnuggets.com/wp-content/uploads/skflow.jpg)\n",
    "\n",
    "## An example similar to Sklearn\n",
    "We will start with creating a model that is similar to what we have seen in scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/9q/yt4vs1hd735gdq1f_yrg4kb40000gp/T/tmppecmmi8f\n",
      "WARNING:tensorflow:From /Users/vantran/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "CPU times: user 5.54 s, sys: 92.4 ms, total: 5.63 s\n",
      "Wall time: 5.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we need to tell tensorflow how many inputs to expect and what the data types will be\n",
    "# for this early example, everything is just numeric, real valued\n",
    "features_tf = [layers.real_valued_column('', dimension=X_train.shape[1])]\n",
    "clf = SKCompat(# wrap with SKCompat for easy usage like sklearn\n",
    "            learn.DNNClassifier(hidden_units=[50], feature_columns=features_tf)\n",
    "        )\n",
    "\n",
    "clf.fit(X_train,y_train,steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 136   0]\n",
      " [  0 497  29]\n",
      " [  0  42  68]] 0.731865284974\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "\n",
    "yhat = clf.predict(X_test)\n",
    "# notice that the output needs some interpretation\n",
    "# as its not completely the same as sklearn\n",
    "yhat = yhat['classes']\n",
    "print(mt.confusion_matrix(y_test,yhat),\n",
    "      mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 136   0]\n",
      " [  0 523   3]\n",
      " [  0  16  94]] 0.799222797927\n",
      "CPU times: user 7.57 s, sys: 425 ms, total: 7.99 s\n",
      "Wall time: 7.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we can also custimize the classifier somewhat:\n",
    "clf = SKCompat(# wrap with SKCompat for easy usage like sklearn\n",
    "            learn.DNNClassifier(hidden_units=[50], \n",
    "                                feature_columns=features_tf,\n",
    "                                activation_fn=tf.nn.sigmoid \n",
    "                                # tf.tanh, tf.sigmoid, tf.nn.relu, tf.nn.softmax etc.\n",
    "                                )\n",
    "        )\n",
    "\n",
    "clf.fit(X_train,y_train,steps=1000)\n",
    "\n",
    "yhat = clf.predict(X_test)['classes']\n",
    "print(mt.confusion_matrix(y_test,yhat),\n",
    "      mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Adding more customization\n",
    "This type of architecture is fine to use, but suffers from many limitations: if its not implemented by the `learn` API, then we can't use the architecture or customize it easily. To get around this, we, instead, will use the `Estimator` class. To use this class, we need to define a \"model function\" that creates the neural network architecture. The output of this model needs to be very specific: a three tuple of \n",
    "- (1) predictions in a format that the end user will need to use\n",
    "- (2) loss function implemented as tf graph \n",
    "- (3)  the optimization operation to perform on the graph. \n",
    "\n",
    "For this particular optimization, let's go beyond SGD and use Adagrad. There are many excellent explanations of different optimizers, for instance:\n",
    "- http://sebastianruder.com/optimizing-gradient-descent/\n",
    "\n",
    "There is a lot of code here that might be new, but essentially we are starting to get down to the core functionality of tensorflow (even though still using the learn API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# let's start by just using the tflearn library out of the box on the data we have\n",
    "def my_model(features, targets, mode):\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    # =====SETUP ARCHITECTURE=====\n",
    "    # we can use functions from learn to add layers and complexity to the model\n",
    "    # pass features through one hidden layer with relu activation\n",
    "    features = layers.relu(features, num_outputs=50) \n",
    "    # now pass the features through a fully connected layer\n",
    "    features = layers.fully_connected(features, num_outputs=1) \n",
    "    # and pass them through a sigmoid activation\n",
    "    output_layer = tf.sigmoid(features) \n",
    "    # reshape the output to be one dimensional\n",
    "    predictions = tf.reshape(output_layer, [-1])\n",
    "    \n",
    "    # depending on the mode, we may not want to evaluate these\n",
    "    loss_mse = None\n",
    "    train_op = None\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        # =====LOSS=======\n",
    "        # we want to use MSE as our loss function, but could also choose \n",
    "        # cross entropy, or other objective functions here\n",
    "        loss_mse = tf.losses.mean_squared_error(targets, predictions) \n",
    "    \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        # =====OPTIMIZER PARAMS========\n",
    "        # now let's setup how we want thing to optimize \n",
    "        train_op = layers.optimize_loss(\n",
    "            loss=loss_mse, \n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            optimizer='Adagrad', # adaptive gradient, so that the learning rate is not SO important\n",
    "            learning_rate=0.1)\n",
    "    \n",
    "    # what format to have the output in when calling clf.predict?\n",
    "    predictions_out = predictions>0.5\n",
    "    return model_fn_lib.ModelFnOps(\n",
    "      mode=mode, predictions=predictions_out, loss=loss_mse, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the model, its as simple as using it in a very familiar syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/9q/yt4vs1hd735gdq1f_yrg4kb40000gp/T/tmpg6tpdj5j\n",
      "[[  0 136   0]\n",
      " [  0 526   0]\n",
      " [  0   9 101]] 0.812176165803\n",
      "CPU times: user 10.4 s, sys: 1.73 s, total: 12.1 s\n",
      "Wall time: 8.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# now let train the estimator\n",
    "# we can wrap the tf estimator in the SKCompat class\n",
    "# so that we can use similar syntax to to SKLearn \n",
    "clf = SKCompat(learn.Estimator(model_fn=my_model))\n",
    "clf.fit(X_train, y_train, steps=5000, batch_size=32)\n",
    "\n",
    "yhat = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test,yhat), \n",
    "      accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Note [start review here]\n",
    "\n",
    "## Handling different feature types\n",
    "\n",
    "You may have noticed that we were not handling the categorical features correctly. Really we need to one hot encode the categorical features\n",
    "unfortunately, this requires using part of the API that breaks some conventions of SKLearn. In this example, we will use the integer categorical labels to get one hot encoded examples.\n",
    "\n",
    "The advantage of this method is that we can write our custom preprocessing steps and use them straight in the pipeline of the classifier. However, the syntax for this is verbose and fairly ugly. So, let's get started!\n",
    "\n",
    "First, we need a process input function that performs all the tensoflow preparation of the data and returns a dictionary of the name and values of each feature column. It should also return the target column as a tf.constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's start with the TF example (manipulated to work with new syntax)\n",
    "# https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "def process_input(df, label_header, categ_headers, numeric_headers):\n",
    "    # input: what ever you need it to be\n",
    "    # output: (dict of feature columns as tensors), (labels as tensors)\n",
    "    \n",
    "    # ========Process Inputs=========\n",
    "    # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "    # the values of that column stored in a constant Tensor.\n",
    "    continuous_cols = {k: tf.expand_dims( # make it a column vector\n",
    "                            tf.cast( # cast to a float32\n",
    "                                tf.constant(df[k].values), \n",
    "                                tf.float32), \n",
    "                            1)\n",
    "                       for k in numeric_headers}\n",
    "    \n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored as constant Tensors (numeric)\n",
    "    # then use tensor flow to one hot encode them using the given number of classes \n",
    "    # name of encoder is **_int need to map only to **\n",
    "    categorical_cols = {k: tf.one_hot(indices=tf.constant(df[k].values),\n",
    "                                      depth=len(encoders[k[:-4]].classes_)) \n",
    "                        for k in categ_headers}\n",
    "    \n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(continuous_cols)\n",
    "    feature_cols.update(categorical_cols)\n",
    "    \n",
    "    # Convert the label column into a constant Tensor.\n",
    "    label = None\n",
    "    if label_header is not None:\n",
    "        label = tf.constant(df[label_header].values)\n",
    "        \n",
    "    return feature_cols, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we want to use the classifier, we tell the model how it needs to parse the inputs. For each numeric feature, we simply need to convert the tensors into column vectors (see below). For the categrocial vectors, TF has a convenient \"one_hot\" function that we can use. The remainder of the model selection stays the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def my_model(dict_features, targets, mode):\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    #=======DECODE FEATURES================\n",
    "    # now let's combine the tensors from the input dictionary\n",
    "    # into a list of the feature columns\n",
    "    features = []\n",
    "    #features = [dict_features[x] for x in numeric_headers+categorical_headers_ints]\n",
    "    for col in numeric_headers:\n",
    "        features.append(dict_features[col])\n",
    "    \n",
    "    # also add in the one hot encoded features\n",
    "    for col in categorical_headers_ints:\n",
    "        features.append(dict_features[col])\n",
    "    \n",
    "    # now we can just combine all the features together\n",
    "    features = tf.concat(values=features,axis=1)\n",
    "    \n",
    "    # =====SETUP ARCHITECTURE=====\n",
    "    # we can use functions from learn to add layers and complexity to the model\n",
    "    # pass features through one hidden layer with relu activation\n",
    "    features = layers.relu(features, num_outputs=50) \n",
    "    # now pass the features through a fully connected layer\n",
    "    features = layers.fully_connected(features, num_outputs=1) \n",
    "    # and pass them through a sigmoid activation\n",
    "    output_layer = tf.sigmoid(features) \n",
    "    # reshape the output to be one dimensional\n",
    "    predictions = tf.reshape(output_layer, [-1])\n",
    "    \n",
    "    # depending on the mode, we may not want to evaluate these\n",
    "    loss_mse = None\n",
    "    train_op = None\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        # =====LOSS=======\n",
    "        # we want to use MSE as our loss function\n",
    "        loss_mse = tf.losses.mean_squared_error(targets, predictions) \n",
    "    \n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        # =====OPTIMIZER PARAMS========\n",
    "        # now let's setup how we want thing to optimize \n",
    "        train_op = layers.optimize_loss(\n",
    "            loss=loss_mse, \n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            optimizer='Adagrad', # adaptive gradient, so that the learning rate is not SO important \n",
    "            learning_rate=0.1)\n",
    "    \n",
    "    # what format to have the output in when calling clf.predict?\n",
    "    predictions_out = predictions>0.5\n",
    "    \n",
    "    return model_fn_lib.ModelFnOps(\n",
    "      mode=mode, predictions={'incomes':predictions_out}, loss=loss_mse, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to call the estimator with our custom pre-processing step we need to wrap the preprocessing inside another function. This is easy to do with a simple lambda, but you could also use another wrapper function that takes no inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/9q/yt4vs1hd735gdq1f_yrg4kb40000gp/T/tmpm9_qo26c\n",
      "CPU times: user 6.29 s, sys: 284 ms, total: 6.57 s\n",
      "Wall time: 5.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = learn.Estimator(model_fn=my_model)\n",
    "\n",
    "# when we provide the process function, they expect us to control the mini-batch\n",
    "clf.fit(input_fn=\n",
    "        lambda:process_input(df_train,'Spendingongadgets',categorical_headers_ints, numeric_headers), \n",
    "        steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 136   0]\n",
      " [  0 384 142]\n",
      " [  0   0 110]] 0.639896373057\n"
     ]
    }
   ],
   "source": [
    "yhat = clf.predict(input_fn=\n",
    "                   lambda:process_input(df_test,None,categorical_headers_ints, numeric_headers))\n",
    "# the output is now an iterable value, so we need to step over it\n",
    "yhat = [x['incomes'] for x in yhat]\n",
    "print(confusion_matrix(y_test,yhat),\n",
    "      accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the confusion matrix is doing pretty well! But we still are just using an MLP with one hidden layer. We really want to take advantage of the crossed columns and embeddings that are possible with tensorflow. That takes yet another syntax change in how we create out tensorflow object. \n",
    "\n",
    "### Note: ended wide/deep lecture here. \n",
    "\n",
    "## [Back to Slides]\n",
    "\n",
    "![asdfasfd](https://www.tensorflow.org/images/wide_n_deep.svg)\n",
    "____\n",
    "# Adding Crossed Columns\n",
    "For this example, we are going to combine our `learn` syntax with the syntax we have just gone over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets create a wide model \n",
    "# https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "def process_input(df, label_header, categ_headers, numeric_headers):\n",
    "    # input: what ever you need it to be\n",
    "    # output: (dict of feature columns as tensors), (labels as tensors)\n",
    "    \n",
    "    # ========Process Inputs=========\n",
    "    # not much changes here, except we leave the numerics as tc.constants\n",
    "    continuous_cols = {k: tf.constant(df[k].values) for k in numeric_headers}\n",
    "      \n",
    "    # and we shift these tensors to be sparse one-hot encoded values\n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "                              indices=[[i, 0] for i in range(df[k].size)],\n",
    "                              values=df[k].values,\n",
    "                              dense_shape=[df[k].size, 1])\n",
    "                        for k in categ_headers}\n",
    "    \n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(categorical_cols)\n",
    "    feature_cols.update(continuous_cols)\n",
    "    \n",
    "    # Convert the label column into a constant Tensor.\n",
    "    label = None\n",
    "    if label_header is not None:\n",
    "        label = tf.constant(df[label_header].values)\n",
    "        \n",
    "    return feature_cols, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_wide_columns():\n",
    "    # let's create the column structure that the learn API can expect\n",
    "    \n",
    "    wide_columns = []\n",
    "    # add in each of the categorical columns\n",
    "    for col in categorical_headers:\n",
    "        wide_columns.append(layers.sparse_column_with_keys(col, keys=encoders[col].classes_))\n",
    "        \n",
    "    # also add in some specific crossed columns\n",
    "    cross_columns = [('Smoking','Alcohol')]\n",
    "    for tup in cross_columns:\n",
    "        wide_columns.append(\n",
    "            layers.crossed_column(\n",
    "                [layers.sparse_column_with_keys(tup[0], keys=encoders[tup[0]].classes_),\n",
    "                 layers.sparse_column_with_keys(tup[1], keys=encoders[tup[1]].classes_)],\n",
    "            hash_bucket_size=int(1e4))\n",
    "        )\n",
    "                        \n",
    "    return wide_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Smoking', 'Alcohol', 'Gender', 'Leftrighthanded', 'Education', 'Villagetown', 'Houseblockofflats']\n",
      "['Music', 'Dance', 'Folk', 'Country', 'Classicalmusic', 'Musical', 'Pop', 'Rock', 'MetalorHardrock', 'Punk', 'HiphopRap', 'ReggaeSka', 'Swing, Jazz', 'Rocknroll', 'Alternative', 'Latino', 'TechnoTrance', 'Opera', 'Movies', 'Horror', 'Thriller', 'Comedy', 'Romantic', 'Scifi', 'War', 'Fantasy', 'Animated', 'Documentary', 'Western', 'Action', 'History', 'Psychology', 'Politics', 'Mathematics', 'Physics', 'Internet', 'PC', 'EconomyManagement', 'Biology', 'Chemistry', 'Reading', 'Geography', 'Foreignlanguages', 'Medicine', 'Law', 'Cars', 'Artexhibitions', 'Religion', 'outdoors', 'Dancing', 'Musicalinstruments', 'Writing', 'Passivesport', 'Activesport', 'Gardening', 'Celebrities', 'Shopping', 'Scienceandtechnology', 'Theatre', 'Funwithfriends', 'Adrenalinesports', 'Pets', 'Healthyeating', 'Age', 'Height', 'Weight', 'Numberofsiblings', 'Spendingongadgets']\n"
     ]
    }
   ],
   "source": [
    "print(categorical_headers)\n",
    "print(numeric_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 134   2]\n",
      " [  0 517   9]\n",
      " [  0 103   7]] 0.678756476684\n",
      "CPU times: user 19.8 s, sys: 864 ms, total: 20.6 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ignore all the deprecations that the learn API needs to deal with... ugh\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# setup \n",
    "wide_columns = setup_wide_columns()\n",
    "input_wrapper = lambda:process_input(df_train,'Spendingongadgets',categorical_headers, numeric_headers)\n",
    "output_wrapper = lambda:process_input(df_test,None,categorical_headers, numeric_headers)\n",
    "\n",
    "clf = learn.LinearClassifier(feature_columns=wide_columns)\n",
    "\n",
    "# when we provide the process function, they expect us to control the mini-batch\n",
    "clf.fit(input_fn=input_wrapper, steps=300)\n",
    "\n",
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "# the output is now an iterable value, so we need to step over it\n",
    "yhat = [x for x in yhat]\n",
    "print(confusion_matrix(y_test,yhat),accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That is just using crossed columns and a one layer Linear Classifer! So memorization works fairly well here. \n",
    "\n",
    "Now let's try a deeper architecture with dense embeddings for the categorical features, as described in lecture.\n",
    "\n",
    "___\n",
    "\n",
    "## Using Dense embeddings in a deeper network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_deep_columns():\n",
    "    # now make up the deep columns\n",
    "    \n",
    "    deep_columns = []\n",
    "    # add in each of the categorical columns to both wide and deep features\n",
    "    for col in categorical_headers:\n",
    "        \n",
    "        tmp = layers.sparse_column_with_keys(col, keys=encoders[col].classes_)\n",
    "        \n",
    "        deep_columns.append(\n",
    "            layers.embedding_column(tmp, dimension=8)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # and add in the regular dense features \n",
    "    for col in numeric_headers:\n",
    "        deep_columns.append(\n",
    "            layers.real_valued_column(col)\n",
    "        )\n",
    "                    \n",
    "    return deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# setup deep columns\n",
    "deep_columns = setup_deep_columns()\n",
    "clf = learn.DNNClassifier(feature_columns=deep_columns, hidden_units=[100, 50])\n",
    "\n",
    "clf.fit(input_fn=input_wrapper, steps=300)\n",
    "\n",
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "# the output is now an iterable value, so we need to step over it\n",
    "yhat = [x for x in yhat]\n",
    "print(confusion_matrix(y_test,yhat),\n",
    "      accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the DNN with feature embeddings is also fairly capable (best thus far). For the final example, lets combine the classifiers together like we talked about in lecture.\n",
    "___\n",
    "\n",
    "## Combining Crossed Linear Classifier and Deep Embeddings\n",
    "Now its just a matter of setting the wide and deep columns for tensorflow. After which, we can use the combined classifier which is already implemented! `learn.DNNLinearCombinedClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_wide_deep_columns():\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    wide_columns = []\n",
    "    deep_columns = []\n",
    "    # add in each of the categorical columns to both wide and deep features\n",
    "    for col in categorical_headers:\n",
    "        wide_columns.append(\n",
    "            layers.sparse_column_with_keys(col, keys=encoders[col].classes_)\n",
    "        )\n",
    "        \n",
    "        dim = round(np.log2(len(encoders[col].classes_)))\n",
    "        deep_columns.append(\n",
    "            layers.embedding_column(wide_columns[-1], dimension=dim)\n",
    "        )\n",
    "        \n",
    "    # also add in some specific crossed columns\n",
    "    cross_columns = [('education','occupation'),('country', 'occupation')]\n",
    "    for tup in cross_columns:\n",
    "        wide_columns.append(\n",
    "            layers.crossed_column(\n",
    "                [layers.sparse_column_with_keys(tup[0], keys=encoders[tup[0]].classes_),\n",
    "                 layers.sparse_column_with_keys(tup[1], keys=encoders[tup[1]].classes_)],\n",
    "            hash_bucket_size=int(1e4))\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # and add in the regular dense features \n",
    "    for col in numeric_headers:\n",
    "        deep_columns.append(\n",
    "            layers.real_valued_column(col)\n",
    "        )\n",
    "                    \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wide_columns, deep_columns = setup_wide_deep_columns()\n",
    "clf = learn.DNNLinearCombinedClassifier(\n",
    "                        linear_feature_columns=wide_columns,\n",
    "                        dnn_feature_columns=deep_columns,\n",
    "                        dnn_hidden_units=[100, 50])\n",
    "\n",
    "\n",
    "clf.fit(input_fn=input_wrapper, steps=2500)\n",
    "\n",
    "yhat = clf.predict(input_fn=output_wrapper)\n",
    "# the output is now an iterable value, so we need to step over it\n",
    "yhat = [x for x in yhat]\n",
    "print(confusion_matrix(y_test,yhat),accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have excellent performance by manipulating the wide and deep architectures in the census data! Excellent!!\n",
    "\n",
    "Wide and deep models can have really interesting and useful properties so they are great to keep in mind when selecting an architecture. Some of the hyperparameters that are specific to this are:\n",
    "- which features to cross together, typically you only want to cross columns you think are important to be connected--they somehow might create new knowledge by combining.\n",
    "- the size of the dense feature embeddings. This can be difficult to set, but one common setting is $log_2(N)$ where $N$ is the total number of uniques values.\n",
    "\n",
    "Also, here are some other references that use many of the same steps as I do:\n",
    "- using custom models via `learn.Estimator`: https://www.tensorflow.org/extend/estimators\n",
    "- many of the same things we went over: https://www.tensorflow.org/tutorials/wide_and_deep \n",
    " - and the Github: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/ops/sparse_feature_cross_op.py \n",
    "- optimizers: http://sebastianruder.com/optimizing-gradient-descent/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Bonus: Working with monitors\n",
    "Much of this example is taken from: \n",
    "\n",
    "- https://www.tensorflow.org/get_started/monitors\n",
    "\n",
    "On my machine, this logging makes the training extremely slow. I haven't taken the time to really track down why that is. However, the outputs are extremely useful. For instance, you can look at the graph structure and loss over time:\n",
    "\n",
    "![tb](data/tensorboard.png)\n",
    "\n",
    "![tb](data/ts_loss.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# input_wrapper = lambda:process_input(df_train,'income',categorical_headers, numeric_headers)\n",
    "# output_wrapper = lambda:process_input(df_test,'income',categorical_headers, numeric_headers)\n",
    "\n",
    "# # for tensorboard this means you can use the following:\n",
    "# # !python -m tensorflow.tensorboard --logdir=large_data/tmp/\n",
    "\n",
    "# wide_columns, deep_columns = setup_wide_deep_columns()\n",
    "# clf = learn.DNNLinearCombinedClassifier(\n",
    "#                         linear_feature_columns=wide_columns,\n",
    "#                         dnn_feature_columns=deep_columns,\n",
    "#                         dnn_hidden_units=[100, 50], \n",
    "#                         model_dir=\"large_data/tmp/\")\n",
    "\n",
    "# validation_metrics = {\n",
    "#     \"accuracy\":\n",
    "#         learn.MetricSpec(\n",
    "#             metric_fn=tf.contrib.metrics.streaming_accuracy,\n",
    "#             prediction_key=learn.PredictionKey.\n",
    "#             CLASSES),\n",
    "#     \"precision\":\n",
    "#         learn.MetricSpec(\n",
    "#             metric_fn=tf.contrib.metrics.streaming_precision,\n",
    "#             prediction_key=learn.PredictionKey.\n",
    "#             CLASSES),\n",
    "#     \"recall\":\n",
    "#         learn.MetricSpec(\n",
    "#             metric_fn=tf.contrib.metrics.streaming_recall,\n",
    "#             prediction_key=learn.PredictionKey.\n",
    "#             CLASSES)\n",
    "# }\n",
    "\n",
    "# validation_monitor = learn.monitors.ValidationMonitor(\n",
    "#     input_fn= lambda:process_input(df_test,'income',categorical_headers, numeric_headers),\n",
    "#     every_n_steps=50,\n",
    "#     metrics=validation_metrics)\n",
    "\n",
    "# clf.fit(input_fn=input_wrapper, \n",
    "#         steps=500, \n",
    "#         monitors=[validation_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "\n",
    "![keras](https://blog.keras.io/img/keras-tensorflow-logo.jpg)\n",
    "\n",
    "\n",
    "# [Optional, Bonus] Using Keras In place of TF Contrib\n",
    "Because we have no idea how things will change in tensorflow and the contribution library once keras is added to the core tensorflow library.\n",
    "\n",
    "This example does not use the wide and deep embeddings becuase keras does not currently have an interface for that type of creation (although you likely can find a way to incorporate the crossed columns and embeddings via the  tensorflow backend). However, if the values can be placed within memory, then the manipulations can occur in sklearn only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# but we were dealing with the data incorrectly because we didn't one hot encode the \n",
    "#   categorical features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# now let's encode the integerr outputs as one hot encoded labels\n",
    "ohe = OneHotEncoder()\n",
    "X_train_ohe = ohe.fit_transform(df_train[categorical_headers_ints].values)\n",
    "X_test_ohe = ohe.transform(df_test[categorical_headers_ints].values)\n",
    "\n",
    "# the ohe instance will help us to organize our encoded matrix\n",
    "print(ohe.feature_indices_)\n",
    "print(X_train_ohe.shape)\n",
    "print(type(X_train_ohe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Embedding, Flatten, Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# combine the features into a single \n",
    "X_train = np.hstack((X_train_ohe.todense(),df_train[numeric_headers].values))\n",
    "X_test = np.hstack((X_test_ohe.todense(),df_test[numeric_headers].values))\n",
    "\n",
    "model = Sequential() # this defines a generic empty model\n",
    "# let's first define the inputs\n",
    "model.add(Dense(input_dim=X_train.shape[1], units=10, activation='relu')) \n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train, epochs=10, batch_size=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "yhat = np.round(model.predict(X_test))\n",
    "print(mt.confusion_matrix(y_test,yhat),mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras with Embeddings\n",
    "Now lets add some embeddings into Keras. Right now the \"sparse\" vector integration in Keras is lacking support, so it can't take advantage of the tensorflow backend sparse representation (yet, it will). As such, we will need to perform our own one hot encoding and then save the matrix as \"dense.\" This wastes memory, but is okay for smaller problems like this one (we only have about 30,000 training examples).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we need to create separate sequential models for each embedding\n",
    "embed_branches = []\n",
    "X_ohe_train = []\n",
    "X_ohe_test = []\n",
    "for col in categorical_headers_ints:\n",
    "    # encode as sparse\n",
    "    ohe = OneHotEncoder()\n",
    "    X_ohe_train.append( ohe.fit_transform(df_train[col].values.reshape((-1,1))).todense() )\n",
    "    X_ohe_test.append(  ohe.transform(df_test[col].values.reshape((-1,1))).todense() )\n",
    "    \n",
    "    # get the number of categories\n",
    "    N = int(X_ohe_train[-1].shape[1]) # same as the max(df_train[col])\n",
    "    \n",
    "    # create embedding branch from the number of categories\n",
    "    embed_branches.append(Sequential())\n",
    "    embed_branches[-1].add( Embedding(N*2, int(np.sqrt(N)), input_length=N) )\n",
    "    embed_branches[-1].add( Flatten() )\n",
    "\n",
    "# also get a dense branch of the numeric features\n",
    "numeric_branch = Sequential()\n",
    "numeric_branch.add( Dense(input_dim=df_train[numeric_headers].values.shape[1], units=20, activation='relu') ) \n",
    "numeric_branch.add( Dense(units=10,activation='relu') )\n",
    "\n",
    "# merge the branches together\n",
    "final_branch = Sequential()\n",
    "final_branch.add( Merge(embed_branches+[numeric_branch], mode='concat') )\n",
    "final_branch.add( Dense(units=1,activation='sigmoid') )\n",
    "\n",
    "final_branch.compile(optimizer='adagrad',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_branch.fit(X_ohe_train + [df_train[numeric_headers].values],\n",
    "        y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yhat = np.round(final_branch.predict(X_ohe_test+[df_test[numeric_headers].values]))\n",
    "print(mt.confusion_matrix(y_test,yhat),mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above was slightly different than the deep embedding that we created from `tf.contrib`. As such, it was able to gracefully combine a layer added to the numeric features branch (which then merged with the other dense embeddings of sparse columns).\n",
    "\n",
    "\n",
    "# Keras crossed columns\n",
    "Unfortunately this is where Keras does not have good support. The crossed embeddings are impossible to create automatically. Therefore, the non-sparse representation will affect memory footprint and run time to the extent that its (probably) not worth doing. Keras will eventually get support for using sparse tensors and probably will implement a crossed column based on Tensorflow, but that is not yet the case. \n",
    "\n",
    "If you really want to implement it in Keras, then look into sklearn's PolynomialFeatures preprocess step to create the dense crossings. \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "\n",
    "You will want to incorporate the `interaction_only` variables to get the crossed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
